{"files":{"202210061132.md":{"filePath":"202210061132.md","links":[],"meta":{"page":{"description":"A collection of techniques or a toolbox to have a programme be determined by data."},"tags":[]},"parentNote":"index.md","title":"Machine Learning","url":"202210061132.html"},"202210061140.md":{"filePath":"202210061140.md","links":[{"resolvedRelTarget":{"contents":"202210171325.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210171325.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210171325.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210171325.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210171323.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210171323.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061205.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061205.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A representation of some real world phenomenom, usually lacking some form of accuracy. Essentially it’s just a function: \n\\bold{y} = f(\\bold{x})\n Where \\bold{x} = [x_1, x_2, ..., x_n] is the input data, features etc. and \\bold{y} = [y_1, y_2, ..., y_n] are the outputs, predictions, inference etc."},"tags":[]},"parentNote":"index.md","title":"Models","url":"202210061140.html"},"202210061205.md":{"filePath":"202210061205.md","links":[{"resolvedRelTarget":{"contents":"202210061507.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061507.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Having too many parameters in the model is in some ways cheating, makes the model too flexible. Can fit the data better without having any better explanatory power."},"tags":[]},"parentNote":"index.md","title":"Overfitting","url":"202210061205.html"},"202210061215.md":{"filePath":"202210061215.md","links":[{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061140.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061140.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Finding the best parameters for a model."},"tags":[]},"parentNote":"index.md","title":"Fitting","url":"202210061215.html"},"202210061216.md":{"filePath":"202210061216.md","links":[{"resolvedRelTarget":{"contents":"202210061228.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061228.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The heart of machine learning. Tell us how good a model is (or how bad). They provide a measure of badness that allow us to optimise it. Might look something like \nL(\\bold{x}, \\bold{y}, \\theta, f)\n Essentially they are a proxy to the task that we want to perform. Absolute value is rarely important, much more relevant is the relative value."},"tags":[]},"parentNote":"index.md","title":"Loss Functions","url":"202210061216.html"},"202210061228.md":{"filePath":"202210061228.md","links":[{"resolvedRelTarget":{"contents":"202210061257.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061257.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061256.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061256.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061255.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061255.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061251.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061251.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061238.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061238.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Want to find a parameter set \\theta^{*} such that L is at its lowest, or \n\\theta^{*} = \\underset{\\theta}\\argmin \\enspace L(\\bold{x}, \\bold{y}, \\theta, f)\n"},"tags":[]},"parentNote":"index.md","title":"Optimisation","url":"202210061228.html"},"202210061238.md":{"filePath":"202210061238.md","links":[],"meta":{"page":{"description":"Looking for the minimum of a loss function. Recall that the derivatives of a function are 0 at its extremes, its minimum and maximum. Can describe the gradient of a loss function L with respect to the parameters \\theta as \\nabla _\\theta L. Sometimes, albeit rarely, you’ll be able to solve \\nabla\n_\\theta L = 0, which gives us the desired \\theta. Often the equation has no solutions."},"tags":[]},"parentNote":"index.md","title":"Analytic Optimisation","url":"202210061238.html"},"202210061251.md":{"filePath":"202210061251.md","links":[],"meta":{"page":{"description":"If the loss function is differentiable, even if there is no analytic solution for finding the minimum, can find it numerically by finding the gradient and moving small increments until a minimum is found. This is known as gradient descent and is often the tool used by ml programs."},"tags":[]},"parentNote":"index.md","title":"Numerical Optimisation","url":"202210061251.html"},"202210061255.md":{"filePath":"202210061255.md","links":[],"meta":{"page":{"description":"If there are finite number of possible values that the parameters can take on, then all the values can be calculated and a minimum can be retrieved. Often this is too expensive in terms of computing resources."},"tags":[]},"parentNote":"index.md","title":"Brute Force Optimisation","url":"202210061255.html"},"202210061256.md":{"filePath":"202210061256.md","links":[],"meta":{"page":{"description":"Divide up the space in a systematic way and then run through the calculations, eg. by grid dimensionality."},"tags":[]},"parentNote":"index.md","title":"Systematic Optimisation","url":"202210061256.html"},"202210061257.md":{"filePath":"202210061257.md","links":[],"meta":{"page":{"description":"Sample all the different dimensions independently. Relevant for ensemble methods, where multiple models are generated and the average or some combined value is taken."},"tags":[]},"parentNote":"index.md","title":"Random Optimisation","url":"202210061257.html"},"202210061301.md":{"filePath":"202210061301.md","links":[],"meta":{"page":{"description":"Fitting a model involves making choices, eg how many nearest neighbours, how many trees in a random forest. These are not parameters of a model, but parameters of a process used to determine a model and are known as a hyperparameters. They are not learned, they are set by some other means. Having lots of hyperparameters is often considered a disadvantage"},"tags":[]},"parentNote":"index.md","title":"Hyperparameters","url":"202210061301.html"},"202210061315.md":{"filePath":"202210061315.md","links":[{"resolvedRelTarget":{"contents":"202210061320.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061320.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061318.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061318.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Important that we don’t test a model on data that it has already seen. With that in mind, the dataset is often split into two, a training set and a test set. Of course, we need enough data in the training set to properly train the model. Typically it’ll be around 80% of the data in the training set and 20% in the test set."},"tags":[]},"parentNote":"index.md","title":"Types of data","url":"202210061315.html"},"202210061318.md":{"filePath":"202210061318.md","links":[{"resolvedRelTarget":{"contents":"202210111445.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111445.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"For linear models, regression concerns modelling continuous outputs."},"tags":[]},"parentNote":"index.md","title":"Regression","url":"202210061318.html"},"202210061320.md":{"filePath":"202210061320.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Classification","url":"202210061320.html"},"202210061330.md":{"filePath":"202210061330.md","links":[],"meta":{"page":{"description":"Used to represent an identity function in vector form, where the vector has 0 in all dimensions apart from the target dimension, which has a 1."},"tags":[]},"parentNote":"index.md","title":"One-hot Encoding","url":"202210061330.html"},"202210061332.md":{"filePath":"202210061332.md","links":[{"resolvedRelTarget":{"contents":"202210061330.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061330.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A representation of one data space in another. An example being one-hot encoding"},"tags":[]},"parentNote":"index.md","title":"Embedding","url":"202210061332.html"},"202210061340.md":{"filePath":"202210061340.md","links":[],"meta":{"page":{"description":"If the “Y“s are known, ie. the outputs of a model, both in terms of the general kind of output as well as specific values of outputs, then this is known as supervised learning. In other words, you have something available at training time which will tell you if you’re right or wrong."},"tags":[]},"parentNote":"index.md","title":"Supervised Learning","url":"202210061340.html"},"202210061341.md":{"filePath":"202210061341.md","links":[],"meta":{"page":{"description":"Involves cleaning data before using to develop/train/test a model, eg. normalising, centering, standardising. Important to remember that any transformations of training data for training also have to be done to real data at test time."},"tags":[]},"parentNote":"index.md","title":"Preprocessing","url":"202210061341.html"},"202210061347.md":{"filePath":"202210061347.md","links":[{"resolvedRelTarget":{"contents":"202210061351.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061351.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This is when we do not have labels for our training data. The model will have to pull out patterns in the data and use that to try to classify the data."},"tags":[]},"parentNote":"index.md","title":"Unsupervised Learning","url":"202210061347.html"},"202210061351.md":{"filePath":"202210061351.md","links":[],"meta":{"page":{"description":"If labels are present for some portion of the data but a larger amount of data is unlabelled, this is known as semi-supervised learning. The model has to learn from the labelled data and apply the rules to the unlabelled data."},"tags":[]},"parentNote":"index.md","title":"Semi-supervised Learning","url":"202210061351.html"},"202210061352.md":{"filePath":"202210061352.md","links":[],"meta":{"page":{"description":"This trains a proxy task for which the data can be generated automatically."},"tags":[]},"parentNote":"index.md","title":"Self-supervised Learning","url":"202210061352.html"},"202210061353.md":{"filePath":"202210061353.md","links":[],"meta":{"page":{"description":"No training data. Instead there is a training environment and the learning is undertaken by agents which interact with the environment and sees how it behaves. Often used to learn how to play games, train robots, etc."},"tags":[]},"parentNote":"index.md","title":"Reinforcement Learning","url":"202210061353.html"},"202210061504.md":{"filePath":"202210061504.md","links":[],"meta":{"page":{"description":"When the model doesn’t even manage to capture the behaviour for the training data. Could be from not enough data, not enough time to learn, etc. etc."},"tags":[]},"parentNote":"index.md","title":"Underfitting","url":"202210061504.html"},"202210061505.md":{"filePath":"202210061505.md","links":[{"resolvedRelTarget":{"contents":"202210061513.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061513.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061509.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061509.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This can relate to a large number of different things."},"tags":[]},"parentNote":"index.md","title":"Bias","url":"202210061505.html"},"202210061507.md":{"filePath":"202210061507.md","links":[{"resolvedRelTarget":{"contents":"202210111808.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111808.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061205.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061205.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A process of attempting to mitigate overfitting. This encourages fitting of the true regularities of the data, as opposed to random fluctuations. However, there’s no definite way to separate noise from training data."},"tags":[]},"parentNote":"index.md","title":"Regularisation","url":"202210061507.html"},"202210061509.md":{"filePath":"202210061509.md","links":[{"resolvedRelTarget":{"contents":"202210061504.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061504.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061205.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061205.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The process of overfitting or underfitting a model."},"tags":[]},"parentNote":"index.md","title":"Misfitting","url":"202210061509.html"},"202210061513.md":{"filePath":"202210061513.md","links":[{"resolvedRelTarget":{"contents":"202210061504.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061504.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061205.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061205.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Here, bias refers to the tendency of a model to misfit due to model structure or assumptions."},"tags":[]},"parentNote":"index.md","title":"Bias-variance Tradeoff","url":"202210061513.html"},"202210061548.md":{"filePath":"202210061548.md","links":[{"resolvedRelTarget":{"contents":"202210061555.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061555.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The set of all possible outcomes of an experiment, denoted by \\Omega."},"tags":[]},"parentNote":"index.md","title":"Sample space","url":"202210061548.html"},"202210061555.md":{"filePath":"202210061555.md","links":[{"resolvedRelTarget":{"contents":"202210071218.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071218.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071208.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071208.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Two events A and B are disjoint if A \\cap B = \\empty where \\empty is known as the impossible set."},"tags":[]},"parentNote":"index.md","title":"Event","url":"202210061555.html"},"202210061601.md":{"filePath":"202210061601.md","links":[{"resolvedRelTarget":{"contents":"202210061621.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061621.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The collection of events can be thought of as a subcollection \\mathcal{F} of the set of all subsets of \\Omega such that:"},"tags":[]},"parentNote":"index.md","title":"Fields","url":"202210061601.html"},"202210061621.md":{"filePath":"202210061621.md","links":[],"meta":{"page":{"description":"A collection \\mathcal{F} of \\Omega is a \\sigma-field if the following are satisfied:"},"tags":[]},"parentNote":"index.md","title":"\\sigma-field","url":"202210061621.html"},"202210061701.md":{"filePath":"202210061701.md","links":[],"meta":{"page":{"description":"If A and B are disjoint events, then \nP(A \\cup B) = P(A) + P(B), P(\\empty) = 0, P(\\Omega) = 1\n Moreover, \n\\text{if } A_1, A_2, \\ldots, A_n \\text{ are disjoint events, then } P\\left(\\bigcup_{i = 1}^nA_i \\right) = \\sum_{i=1}^{n} P(A_i)\n"},"tags":[]},"parentNote":"index.md","title":"Probability","url":"202210061701.html"},"202210071001.md":{"filePath":"202210071001.md","links":[{"resolvedRelTarget":{"contents":"202210090920.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210090920.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210090914.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210090914.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071009.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071009.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The conditional probability that A occurs given that B occurs is defined as P(A|B) and is the probability of A and B occurring divided by the probability of B, formally \nP(A|B) = \\frac{P(A,B)}{P(B)}\n"},"tags":[]},"parentNote":"index.md","title":"Conditional probability","url":"202210071001.html"},"202210071009.md":{"filePath":"202210071009.md","links":[],"meta":{"page":{"description":"A family B_1, B_2, \\ldots, B_n of events is called a partition of the set \\Omega if all of B_i are disjoint, and their union is the entire set \\Omega."},"tags":[]},"parentNote":"index.md","title":"Partition of a set","url":"202210071009.html"},"202210071208.md":{"filePath":"202210071208.md","links":[],"meta":{"page":{"description":"Events A and B are independent if \nP(A, B) = P(A \\cap B) = P(A)P(B)\n"},"tags":[]},"parentNote":"index.md","title":"Independence","url":"202210071208.html"},"202210071218.md":{"filePath":"202210071218.md","links":[{"resolvedRelTarget":{"contents":"202210071218.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071218.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Events A and B are conditionally independent given C if"},"tags":[]},"parentNote":"index.md","title":"Conditional independence","url":"202210071218.html"},"202210071338.md":{"filePath":"202210071338.md","links":[],"meta":{"page":{"description":"For a set A, the cardinality, denoted by |A| is the number of elements in the set."},"tags":[]},"parentNote":"index.md","title":"Cardinality","url":"202210071338.html"},"202210071404.md":{"filePath":"202210071404.md","links":[],"meta":{"page":{"description":"The number of permutations of n objects is n! and the number of ways of choosing r objects from n is \\binom{n}{r}"},"tags":[]},"parentNote":"index.md","title":"Combinatorics","url":"202210071404.html"},"202210071450.md":{"filePath":"202210071450.md","links":[],"meta":{"page":{"description":"The complement of the union of two sets is equal to the intersection of their complements. \n(A \\cup B)^{c} = A^{c} \\cap B^{c}\n"},"tags":[]},"parentNote":"index.md","title":"De Morgan’s Law","url":"202210071450.html"},"202210071534.md":{"filePath":"202210071534.md","links":[{"resolvedRelTarget":{"contents":"202210101028.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101028.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081010.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081010.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071536.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071536.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A function mapping \\Omega into \\Reals. More precisely, it has the property that \\{\\omega \\in \\Omega: X(\\omega) \\leq x\\} \\in \\mathcal{F} for each x \\in\n\\Reals. Therefore to show that Y is a random variable, need to show that"},"tags":[]},"parentNote":"index.md","title":"Random variables","url":"202210071534.html"},"202210071536.md":{"filePath":"202210071536.md","links":[{"resolvedRelTarget":{"contents":"202210071723.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071723.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071722.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071722.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071534.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071534.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The distribution function of a random variable X is the function F:\\Reals\n\\rightarrow [0,1] given by F(x) = P(X \\leq x)."},"tags":[]},"parentNote":"index.md","title":"Distribution function","url":"202210071536.html"},"202210071722.md":{"filePath":"202210071722.md","links":[],"meta":{"page":{"description":"For a discrete random variable X, the probability mass function is the function f: \\Reals \\rightarrow [0, 1] given by f(x) = P(X = x)."},"tags":[]},"parentNote":"index.md","title":"Probability mass function","url":"202210071722.html"},"202210071723.md":{"filePath":"202210071723.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Probability density function","url":"202210071723.html"},"202210081010.md":{"filePath":"202210081010.md","links":[{"resolvedRelTarget":{"contents":"202210081017.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081017.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081016.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081016.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071536.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071536.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A random variable taking on two possible values, 0 and 1, with a distribution function F(x) = P(X \\leq x): \nF(x) = \\begin{cases}\n0 & x < 0 \\\\\n1 - p & 0 \\leq x \\lt 1 \\\\\n1 & x \\geq 1\n\\end{cases}\n"},"tags":[]},"parentNote":"index.md","title":"Bernoulli variable","url":"202210081010.html"},"202210081016.md":{"filePath":"202210081016.md","links":[{"resolvedRelTarget":{"contents":"202210091028.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091028.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"\n\\begin{align*}\nP(x = 0) &= 1-\\lambda \\\\[0.5em]\nP(x = 1) &= \\lambda \\\\[0.5em]\n\\end{align*}\n or \nP(x) = \\lambda^x(1-\\lambda)^{1-x}\n"},"tags":[]},"parentNote":"index.md","title":"Bernoulli distribution","url":"202210081016.html"},"202210081017.md":{"filePath":"202210081017.md","links":[{"resolvedRelTarget":{"contents":"202210081010.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081010.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A particular class of Bernoulli variable, given by: \nI_A(\\omega) = \\begin{cases}\n1 & \\text{if } \\omega \\in A, \\\\\n0 & \\text{if } \\omega \\in A^{c}\n\\end{cases}\n"},"tags":[]},"parentNote":"index.md","title":"Indicator function","url":"202210081017.html"},"202210081143.md":{"filePath":"202210081143.md","links":[{"resolvedRelTarget":{"contents":"202210081156.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081156.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"X and Y are random variables. When thinking about their properties relative to each other, it’s helpful to thing of them as components of a random vector (X, Y) taking values in \\Reals^2."},"tags":[]},"parentNote":"index.md","title":"Random vectors","url":"202210081143.html"},"202210081156.md":{"filePath":"202210081156.md","links":[{"resolvedRelTarget":{"contents":"202210081143.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081143.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071536.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071536.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The JDF of a random vector is the function F_{\\bold{X}}(\\bold{x}) = P(\\bold{X} \\le \\bold{x}) for \\bold{x} \\in \\Reals^n."},"tags":[]},"parentNote":"index.md","title":"Joint distribution function","url":"202210081156.html"},"202210081358.md":{"filePath":"202210081358.md","links":[{"resolvedRelTarget":{"contents":"202210081016.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081016.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This is for an experiment with two outcomes where a random variable X describes some outcome of the experiment."},"tags":[]},"parentNote":"index.md","title":"Binomial distribution","url":"202210081358.html"},"202210081412.md":{"filePath":"202210081412.md","links":[],"meta":{"page":{"description":"If a random variable X takes values in the set \\{0, 1, 2, \\ldots\\} with mass function \nf(k) = \\frac{\\lambda^k}{k!}e^{-\\lambda}, \\enspace \\enspace k = 0, 1, 2, \\ldots\n Then X is said to have the Poisson distribution."},"tags":[]},"parentNote":"index.md","title":"Poisson distribution","url":"202210081412.html"},"202210081500.md":{"filePath":"202210081500.md","links":[{"resolvedRelTarget":{"contents":"202210081512.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081512.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The mean value, or expectation, or expected value of the random variable X with mass function f is defined to be \n\\mathbb{E}(X) = \\sum_{x:f(x)>0} xf(x)\n"},"tags":[]},"parentNote":"index.md","title":"Expectation","url":"202210081500.html"},"202210081512.md":{"filePath":"202210081512.md","links":[{"resolvedRelTarget":{"contents":"202210081517.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081517.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"2022100815","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"If k is a positive integer, the kth moment m_k of X is defined to be m_k=\\mathbb{E}(X^k). The kth central moment \\sigma_k = \\mathbb{E}((X -\nm_1)^k)."},"tags":[]},"parentNote":"index.md","title":"Moments of a random variable","url":"202210081512.html"},"202210081517.md":{"filePath":"202210081517.md","links":[{"resolvedRelTarget":{"contents":"202210081525.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081525.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This measures the degree to which a random variable deviates from the mean. Often denoted var(X). The positive square root \\sigma = \\sqrt{\\text{var}(X)} is called the standard deviation."},"tags":[]},"parentNote":"index.md","title":"Variance","url":"202210081517.html"},"202210081525.md":{"filePath":"202210081525.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Standard deviation","url":"202210081525.html"},"202210090914.md":{"filePath":"202210090914.md","links":[{"resolvedRelTarget":{"contents":"202210071208.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071208.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"For three random variables A, B, C, we have \n\\begin{align*}\nP(A, B, C) &= P(C|A,B)P(B)P(C) \\\\[0.5em]\n&= P(C)P(A,B|C) \\\\[0.5em]\n&= P(C)P(A|C)P(B|C) \\\\[0.5em]\n\\end{align*}\n assuming P(A,B) = P(A)P(B) (independence)."},"tags":[]},"parentNote":"index.md","title":"Joint probability","url":"202210090914.html"},"202210090920.md":{"filePath":"202210090920.md","links":[],"meta":{"page":{"description":"\n\\begin{align*}\nP(y|x)P(x) &= P(x|y)P(y) \\\\[1em]\nP(y|x) &= \\frac{P(x|y)P(y)}{P(x)} \\medspace \\text{ or } \\medspace \\frac{P(x,y)}{P(x)}\\\\[1em]\n&= \\frac{P(x|y)P(y)}{\\int P(x,y)dy} \\\\[1em]\n&= \\frac{P(x|y)P(y)}{\\int P(x|y)P(y)dy} \\\\[1em]\n\\end{align*}\n"},"tags":[]},"parentNote":"index.md","title":"Bayes’ rule","url":"202210090920.html"},"202210091028.md":{"filePath":"202210091028.md","links":[{"resolvedRelTarget":{"contents":"202210081016.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081016.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061301.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061301.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Defined over data \\lambda \\in [0,1] (i.e. parameter of Bernoulli), i.e. a hyperparameter."},"tags":[]},"parentNote":"index.md","title":"Beta distribution","url":"202210091028.html"},"202210091049.md":{"filePath":"202210091049.md","links":[{"resolvedRelTarget":{"contents":"202210091107.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091107.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091028.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091028.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081016.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081016.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Used for discrete random variables. \nP(x = k) = \\lambda_k\n"},"tags":[]},"parentNote":"index.md","title":"Categorical distribution","url":"202210091049.html"},"202210091107.md":{"filePath":"202210091107.md","links":[{"resolvedRelTarget":{"contents":"202210091049.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091049.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Used to define the hyperparameters for a Categorical distribution."},"tags":[]},"parentNote":"index.md","title":"Dirichlet distribution","url":"202210091107.html"},"202210091114.md":{"filePath":"202210091114.md","links":[{"resolvedRelTarget":{"contents":"202210091117.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091117.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Written as \nP(x) = \\text{Norm}_x[\\overbrace{\\mu}^{\\text{mean}}, \\underbrace{\\sigma^2}_{\\text{variance}}]\n"},"tags":[]},"parentNote":"index.md","title":"Univariate normal distribution (Gaussian)","url":"202210091114.html"},"202210091117.md":{"filePath":"202210091117.md","links":[{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This distribution is defined over a pair of continuous variables \\mu,\n\\sigma^2, the first of which can take any value and the second which is constrained to be positive. It can therefore define a distribution over the mean and variance parameter of a normal distribution."},"tags":[]},"parentNote":"index.md","title":"Normal inverse gamma distribution","url":"202210091117.html"},"202210091450.md":{"filePath":"202210091450.md","links":[{"resolvedRelTarget":{"contents":"202210081156.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081156.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Given a joint distribution p(x,y), the marginal distribution of x is defined by \n\\begin{align*}\np(x) &= \\sum_{y} p(x,y) \\enspace \\text{ for discrete } \\\\[0.5em]\np(x) &= \\int_{y} p(x,y)dy \\enspace \\text{ for continous }  \\\\[0.5em]\n\\end{align*}\n That is, the marginal distribution of one variable is obtained by summing over all the possible values of the other variable. Here, p(x) is termed a marginal of the join probability distribution p(x,y). The process of computing a marginal is called marginalisation. This allows us to recover the probability distribution of any single variable from a joint distribution. In other words, we are finding the probability distribution of x regardless of (or in the absence of information about) the value of y."},"tags":[]},"parentNote":"index.md","title":"Marginals & marginalisation","url":"202210091450.html"},"202210101028.md":{"filePath":"202210101028.md","links":[],"meta":{"page":{"description":"All the possible values the random variable can take on."},"tags":[]},"parentNote":"index.md","title":"Domain of a random variable","url":"202210101028.html"},"202210101307.md":{"filePath":"202210101307.md","links":[{"resolvedRelTarget":{"contents":"202210190852.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210190852.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210131406.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210131406.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210130950.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210130950.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210130950.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210130950.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101311.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101311.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091450.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091450.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071001.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071001.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The PDF is given as:"},"tags":[]},"parentNote":"index.md","title":"Multivariate normal distribution","url":"202210101307.html"},"202210101311.md":{"filePath":"202210101311.md","links":[{"resolvedRelTarget":{"contents":"202210101307.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101307.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This defines a distribution over a D \\times 1 vector \\boldsymbol{\\mu} and a D \\times D positive definite matrix \\boldsymbol{\\Sigma}. It is therefore suitable for describing uncertainty in the parameters of a Multivariate normal distribution."},"tags":[]},"parentNote":"index.md","title":"Normal inverse Wishart distribution","url":"202210101311.html"},"202210101320.md":{"filePath":"202210101320.md","links":[{"resolvedRelTarget":{"contents":"202210101311.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101311.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101307.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101307.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091117.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091117.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091049.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091049.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"2022100911","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"2022100910","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"2022100810","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"These pairs have a special relationship, they are conjugate distributions:"},"tags":[]},"parentNote":"index.md","title":"Conjugate distributions","url":"202210101320.html"},"202210101331.md":{"filePath":"202210101331.md","links":[],"meta":{"page":{"description":"Find the parameters \\boldsymbol{\\hat{\\theta}} under which the data \\bold{x}_{1\\ldots I} are most likely."},"tags":[]},"parentNote":"index.md","title":"Maximum likelihood","url":"202210101331.html"},"202210101339.md":{"filePath":"202210101339.md","links":[{"resolvedRelTarget":{"contents":"202210111029.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111029.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111029.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111029.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091117.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091117.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Here we introduce prior information about the parameters \\boldsymbol{\\theta}. MAP maximizes the posterior probability Pr(\\boldsymbol{\\theta}|\\bold{x}_{1 \\ldots I}) of the parameters."},"tags":[]},"parentNote":"index.md","title":"Maximum a posteriori (MAP)","url":"202210101339.html"},"202210101619.md":{"filePath":"202210101619.md","links":[{"resolvedRelTarget":{"contents":"202210081525.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081525.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"2022100815","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"Starting with some data, asking what are the models that could have generated this data? E.g. maybe we have some data that looks like a Gaussian distribution, what is the mean and the standard deviation."},"tags":[]},"parentNote":"index.md","title":"Statistical inference","url":"202210101619.html"},"202210101627.md":{"filePath":"202210101627.md","links":[],"meta":{"page":{"description":"A summary of the data."},"tags":[]},"parentNote":"index.md","title":"Statistic","url":"202210101627.html"},"202210111029.md":{"filePath":"202210111029.md","links":[],"meta":{"page":{"description":"What you know before observing any data. This can come in the form of historical data, or just some injected knowledge about how the system works."},"tags":[]},"parentNote":"index.md","title":"Prior","url":"202210111029.html"},"202210111043.md":{"filePath":"202210111043.md","links":[{"resolvedRelTarget":{"contents":"202210131442.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210131442.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101339.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101339.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091117.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091117.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Unlike the maximum likelihood and MAP, which seek to estimate the best fitting single fixed values (or point estimates) of the parameters \\boldsymbol{\\theta}, here we instead compute a probability distribution Pr(\\boldsymbol{\\theta}|\\bold{x}_{1 \\ldots I}) with"},"tags":[]},"parentNote":"index.md","title":"Bayesian normal fitting","url":"202210111043.html"},"202210111445.md":{"filePath":"202210111445.md","links":[{"resolvedRelTarget":{"contents":"202210111636.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111636.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111531.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111531.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061216.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061216.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Assumption is all of the model variables are the inputs and the outputs are continuous. The output is a weighted sum of the inputs with a possible intercept term as well."},"tags":[]},"parentNote":"index.md","title":"Linear models","url":"202210111445.html"},"202210111531.md":{"filePath":"202210111531.md","links":[{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"2022101114","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"Convenient to consider the Euclidean distance between the observed output vs the predicted output as a form of loss function, \\|\\bold{y} - \\bold{\\hat{y}}\\|^2. This is known as the Euclidean, or L2, norm. To get our loss function, we sum the norms as (using the notation from linear models): \n\\sum_{n=1}^{n} \\|y_i - \\hat{y}_i\\|^2 = \\sum_{n=1}^{n} \\|y_i - \\bold{x}_i \\cdot\n\\bold{w}\\|^2 = \\|\\bold{X}\\bold{w} - \\bold{y}\\|^2\n"},"tags":[]},"parentNote":"index.md","title":"Residual sum of squares","url":"202210111531.html"},"202210111543.md":{"filePath":"202210111543.md","links":[],"meta":{"page":{"description":"Some useful identities of gradients: ."},"tags":[]},"parentNote":"index.md","title":"Gradient","url":"202210111543.html"},"202210111544.md":{"filePath":"202210111544.md","links":[],"meta":{"page":{"description":"Some useful identities of matrix multiplication:"},"tags":[]},"parentNote":"index.md","title":"Matrix multiplication","url":"202210111544.html"},"202210111636.md":{"filePath":"202210111636.md","links":[{"resolvedRelTarget":{"contents":"202210071534.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071534.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Often a model needs to account for errors, and these errors are often packaged up in an error term, typically denoted with \\epsilon."},"tags":[]},"parentNote":"index.md","title":"Error term","url":"202210111636.html"},"202210111653.md":{"filePath":"202210111653.md","links":[{"resolvedRelTarget":{"contents":"202210111544.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111544.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111543.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111543.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111531.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111531.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This is a method for fitting a linear model. To solve it we solve the Residual sum of squares, i.e. we solve \n\\bold{w}^* = \\argmin_{\\bold{w}} \\|\\bold{X}\\bold{w} - \\bold{y}\\|^2\n"},"tags":[]},"parentNote":"index.md","title":"Ordinary least squares","url":"202210111653.html"},"202210111712.md":{"filePath":"202210111712.md","links":[{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"2022101114","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"Models fitted from transformed data can still be linear models. Any model that can be expressed as a weighted sum of arbitrary fixed functions of the input is a linear model and can be fitted by modelling least squares. This transformation means that these inputs occupy a feature space with a different basis."},"tags":[]},"parentNote":"index.md","title":"Basis expansion","url":"202210111712.html"},"202210111808.md":{"filePath":"202210111808.md","links":[{"resolvedRelTarget":{"contents":"202210111811.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111811.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061507.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061507.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061315.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061315.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A form of regularisation. In this scenario, different models are generating with varying numbers of features. The performance of the models are then tested (using e.g. cross validation, or with a dedicated validation set) and then a criterion that trades off the number of parameters against the performance is applied."},"tags":[]},"parentNote":"index.md","title":"Feature selection","url":"202210111808.html"},"202210111811.md":{"filePath":"202210111811.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Cross validation","url":"202210111811.html"},"202210111821.md":{"filePath":"202210111821.md","links":[{"resolvedRelTarget":{"contents":"202210111653.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111653.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061507.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061507.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061301.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061301.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A form of regularisation. A penalty is applied in the loss function to deter the fitting process from bad actions. \nL(\\bold{X}, \\bold{y};\\bold{\\boldsymbol{\\theta}}) = \\underbrace{D(f_{\\theta}(\\bold{X},\\bold{y}),\\bold{y})}_{\\text{\"badness\" of model}} + \\lambda \\overbrace{P(\\boldsymbol{\\theta})}^{\\text{some penalty}}\n"},"tags":[]},"parentNote":"index.md","title":"Penalisation","url":"202210111821.html"},"202210111828.md":{"filePath":"202210111828.md","links":[{"resolvedRelTarget":{"contents":"202210111821.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111821.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111653.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111653.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A form of penalisation. A penalty applied to the model weights takes the same form as the penalty on the residuals: P(\\bold{w}) =\n\\|\\bold{w}\\|^2. This essentially means, make the weights as small as possible."},"tags":[]},"parentNote":"index.md","title":"Ridge regression","url":"202210111828.html"},"202210111836.md":{"filePath":"202210111836.md","links":[{"resolvedRelTarget":{"contents":"202210111828.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111828.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111821.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111821.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Another form of penalisation. Similar to ridge regression, but instead of the penalty function taking the squared L2 norm, it takes the L1 norm P(\\bold{w}) = \\|\\bold{w}\\|_1, the sum of the absolute values."},"tags":[]},"parentNote":"index.md","title":"Lasso","url":"202210111836.html"},"202210121728.md":{"filePath":"202210121728.md","links":[{"resolvedRelTarget":{"contents":"202210101619.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101619.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Equivalent to inference. Involves identifying all relevant variables x_1, \\ldots, x_N in the environment, and making a probabilistic model p(x_1, \\ldots, x_N) of their interaction. Reasoning is then performed by introducing evidence that sets variables in known states, and subsequently computing probabilities of interest, conditioned on this evidence."},"tags":[]},"parentNote":"index.md","title":"Probabilistic reasoning","url":"202210121728.html"},"202210121758.md":{"filePath":"202210121758.md","links":[],"meta":{"page":{"description":"Two fair dice are rolled. Someone tells you that the sum of the two scores is 9. What is the distribution of the dice scores?"},"tags":[]},"parentNote":"index.md","title":"Posterior distribution","url":"202210121758.html"},"202210130909.md":{"filePath":"202210130909.md","links":[{"resolvedRelTarget":{"contents":"202210111043.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111043.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101339.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101339.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Given some data, we want a model. That involves choosing an appropriate one, eg. Normal, and estimating the parameters. Estimating the parameters can be done in a variety of ways, eg:"},"tags":[]},"parentNote":"index.md","title":"Estimating parameters","url":"202210130909.html"},"202210130950.md":{"filePath":"202210130950.md","links":[{"resolvedRelTarget":{"contents":"202210101307.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101307.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Covariance matrices in multivariate normal distribution take three forms, spherical, diagonal and full:"},"tags":[]},"parentNote":"index.md","title":"Covariance","url":"202210130950.html"},"202210131111.md":{"filePath":"202210131111.md","links":[],"meta":{"page":{"description":"A fully connected subset of nodes - every node is connected to every other node. (X1, X2, X4) forms a clique."},"tags":[]},"parentNote":"index.md","title":"Clique","url":"202210131111.html"},"202210131116.md":{"filePath":"202210131116.md","links":[{"resolvedRelTarget":{"contents":"202210090914.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210090914.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A DAG in which each node is associated with the conditional probability of the node given its parents."},"tags":[]},"parentNote":"index.md","title":"Belief networks (Bayesian networks)","url":"202210131116.html"},"202210131227.md":{"filePath":"202210131227.md","links":[{"resolvedRelTarget":{"contents":"202210071218.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071218.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071208.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071208.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A collider contains two or more incoming arrows along a chosen path."},"tags":[]},"parentNote":"index.md","title":"Collider","url":"202210131227.html"},"202210131406.md":{"filePath":"202210131406.md","links":[],"meta":{"page":{"description":"A matrix X is positive definite if for any \\bold{z}, \\bold{z}^{\\top}X\\bold{z} is positive."},"tags":[]},"parentNote":"index.md","title":"Positive definite","url":"202210131406.html"},"202210131442.md":{"filePath":"202210131442.md","links":[{"resolvedRelTarget":{"contents":"202210111043.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111043.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101339.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101339.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Suppose you have a model, we would like to know the probability of a new datum \\bold{x}^* under the model. This is known as the evaluating the predictive distribution. 3 methods are typically used:"},"tags":[]},"parentNote":"index.md","title":"Predictive distribution","url":"202210131442.html"},"202210141045.md":{"filePath":"202210141045.md","links":[{"resolvedRelTarget":{"contents":"202210141248.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141248.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141123.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141123.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141113.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141113.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111531.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111531.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061318.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061318.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061251.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061251.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Unlike regression for linear models, binary classification is when the outputs fall into 2 classes with either a 0 or 1 value. For example, for 1D binary classification, we could say that if x < 0, then y = 0, and if x > 0, then y = 1."},"tags":[]},"parentNote":"index.md","title":"Binary classification","url":"202210141045.html"},"202210141113.md":{"filePath":"202210141113.md","links":[{"resolvedRelTarget":{"contents":"202210061216.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061216.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A form of loss function with the following form"},"tags":[]},"parentNote":"index.md","title":"Zero-one loss","url":"202210141113.html"},"202210141123.md":{"filePath":"202210141123.md","links":[],"meta":{"page":{"description":"A type of squashing function: \n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n"},"tags":[]},"parentNote":"index.md","title":"Sigmoid (or logistic) function","url":"202210141123.html"},"202210141214.md":{"filePath":"202210141214.md","links":[{"resolvedRelTarget":{"contents":"202210141225.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141225.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141221.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141221.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141216.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141216.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061238.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061238.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"If we can’t find a minimum point analytically, we can move down to it up taking little steps down the gradient. Comes in different forms:"},"tags":[]},"parentNote":"index.md","title":"Gradient descent","url":"202210141214.html"},"202210141216.md":{"filePath":"202210141216.md","links":[{"resolvedRelTarget":{"contents":"202210141221.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141221.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Starting at a random initial position, we calculate the local loss gradient with respect to the entire training dataset. We take a small step, known as a learning rate down the gradient, check if the minimum has been reached, if not then do it all again. Comes with certain problems:"},"tags":[]},"parentNote":"index.md","title":"Batch gradient descent","url":"202210141216.html"},"202210141221.md":{"filePath":"202210141221.md","links":[{"resolvedRelTarget":{"contents":"202210141240.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141240.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141225.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141225.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141216.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141216.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Instead of calculating the loss gradient for the entire training set, you do it in small stages, one sample at a time."},"tags":[]},"parentNote":"index.md","title":"Stochastic gradient descent","url":"202210141221.html"},"202210141225.md":{"filePath":"202210141225.md","links":[{"resolvedRelTarget":{"contents":"202210141221.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141221.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141216.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141216.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Halfway between batch and stochastic, don’t process all samples at the same time, but similarly don’t do just 1 sample at a time. Divide the training set into chunks, then process each chunk at a time. This introduces another hyperparameter, the chunk size."},"tags":[]},"parentNote":"index.md","title":"Mini-batch gradient descent","url":"202210141225.html"},"202210141240.md":{"filePath":"202210141240.md","links":[],"meta":{"page":{"description":"A weighted running tally of recent updates. Can think of this as a velocity"},"tags":[]},"parentNote":"index.md","title":"Momentum","url":"202210141240.html"},"202210141248.md":{"filePath":"202210141248.md","links":[{"resolvedRelTarget":{"contents":"202210141257.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141257.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141255.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141255.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A way of categorising variables into different classes."},"tags":[]},"parentNote":"index.md","title":"Multiclass","url":"202210141248.html"},"202210141255.md":{"filePath":"202210141255.md","links":[{"resolvedRelTarget":{"contents":"202210141045.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141045.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Train up a separate binary classifier for each of the classes classifying that single class against all the samples of all the other classes. Choose the classifier that has the highest prediction score."},"tags":[]},"parentNote":"index.md","title":"One-vs-rest approach","url":"202210141255.html"},"202210141257.md":{"filePath":"202210141257.md","links":[{"resolvedRelTarget":{"contents":"202210141255.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141255.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Similar to the one-vs-rest approach, but instead you build up pairwise classifiers for all the different classes."},"tags":[]},"parentNote":"index.md","title":"One-vs-one approach","url":"202210141257.html"},"202210141357.md":{"filePath":"202210141357.md","links":[],"meta":{"page":{"description":"These models do have parameters, but, unlike linear models, these parameters are not fixed and are instead determined by the data. E.g. the number of parameters and the way they interact are learned."},"tags":[]},"parentNote":"index.md","title":"Non-parametric models","url":"202210141357.html"},"202210141406.md":{"filePath":"202210141406.md","links":[],"meta":{"page":{"description":"Consists taking a sample and comparing it to all of the other samples in the training set, finding some number that it is most like, then using that to predict the output value of that bit of data. Needs some metric of similarity, or distance."},"tags":[]},"parentNote":"index.md","title":"k-Nearest neighbours","url":"202210141406.html"},"202210141453.md":{"filePath":"202210141453.md","links":[],"meta":{"page":{"description":"Want to learn the structure of the tree from the data. Can think of it as a partition of the feature space. There are different ways of learning the structure the tree:"},"tags":[]},"parentNote":"index.md","title":"Decision trees","url":"202210141453.html"},"202210141516.md":{"filePath":"202210141516.md","links":[],"meta":{"page":{"description":"To combat variance in the data, we can aggregate many different models and take an average. Need the models in the ensemble to be different from each other."},"tags":[]},"parentNote":"index.md","title":"Ensembles","url":"202210141516.html"},"202210141517.md":{"filePath":"202210141517.md","links":[{"resolvedRelTarget":{"contents":"202210141516.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141516.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A type of ensembling, involves combining models that aren’t very good, aggregating them and then taking an average."},"tags":[]},"parentNote":"index.md","title":"Boosting","url":"202210141517.html"},"202210141518.md":{"filePath":"202210141518.md","links":[{"resolvedRelTarget":{"contents":"202210141519.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141519.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Combine multiple good models (strong learners). Typically the models will be trained on different sets of the training data. The most popular type of bagging is random forests."},"tags":[]},"parentNote":"index.md","title":"Bagging","url":"202210141518.html"},"202210141519.md":{"filePath":"202210141519.md","links":[{"resolvedRelTarget":{"contents":"202210141516.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141516.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141453.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141453.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"An ensemble of decision trees. A popular choice of model when there is no obvious structure to the data that would drive you to some other choice of model (e.g. the data obviously isn’t linear)."},"tags":[]},"parentNote":"index.md","title":"Random forests","url":"202210141519.html"},"202210150928.md":{"filePath":"202210150928.md","links":[{"resolvedRelTarget":{"contents":"202210151254.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151254.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151146.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151146.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151102.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151102.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151047.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151047.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151020.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151020.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101627.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101627.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081358.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081358.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081010.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081010.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A way to check any assumptions made are reasonable."},"tags":[]},"parentNote":"index.md","title":"Hypothesis testing","url":"202210150928.html"},"202210151020.md":{"filePath":"202210151020.md","links":[{"resolvedRelTarget":{"contents":"202210151142.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151142.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210150928.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210150928.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The p-value is the probability of observing a test statistic, X, as or more extreme than the value x seen in the data, under the assumptions that the null hypothesis, H_0, is true. If we get a very low p-value, it would suggest that it was very unlikely that we received the sample we did based on the assumption that H_0 was true, therefore we have evidence to reject H_0."},"tags":[]},"parentNote":"index.md","title":"p-value","url":"202210151020.html"},"202210151047.md":{"filePath":"202210151047.md","links":[],"meta":{"page":{"description":"In logic, implications may be reversed to provide the contrapositive: \nA \\rArr B\n If A is a statistician, then A is a data scientist"},"tags":[]},"parentNote":"index.md","title":"Contrapositive","url":"202210151047.html"},"202210151102.md":{"filePath":"202210151102.md","links":[{"resolvedRelTarget":{"contents":"202210150928.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210150928.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"For the example in Hypothesis testing:"},"tags":[]},"parentNote":"index.md","title":"Hypothesis testing procedure","url":"202210151102.html"},"202210151142.md":{"filePath":"202210151142.md","links":[{"resolvedRelTarget":{"contents":"202210150928.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210150928.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Hypothesis testing involves a trade-off between two drawbacks:"},"tags":[]},"parentNote":"index.md","title":"Statistical power","url":"202210151142.html"},"202210151146.md":{"filePath":"202210151146.md","links":[{"resolvedRelTarget":{"contents":"202210151142.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151142.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151020.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151020.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"We may define a rule under which we reject H_0, e.g. if the probability of obtaining an outcome as or more extreme than that observed is < or = to 0.05 under the assumption that H_0 is true."},"tags":[]},"parentNote":"index.md","title":"Setting a significance level","url":"202210151146.html"},"202210151254.md":{"filePath":"202210151254.md","links":[{"resolvedRelTarget":{"contents":"202210150928.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210150928.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"If we make stronger assumptions with the consequence of reducing our set of alternatives we may increase the power of our test."},"tags":[]},"parentNote":"index.md","title":"Selecting a hypothesis","url":"202210151254.html"},"202210151321.md":{"filePath":"202210151321.md","links":[],"meta":{"page":{"description":"A null hypothesis can postulate more than one value for the target state of nature, for example: \nH_0: \\theta \\ge 0.5 \\\\[0.5em]\nH_1: \\theta < 0.5 \\\\[0.5em]\n"},"tags":[]},"parentNote":"index.md","title":"Composite hypothesis","url":"202210151321.html"},"202210151447.md":{"filePath":"202210151447.md","links":[{"resolvedRelTarget":{"contents":"202210150928.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210150928.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101627.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101627.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A form of hypothesis testing, motivated by small sample sizes, normally distributed."},"tags":[]},"parentNote":"index.md","title":"T-test","url":"202210151447.html"},"202210151505.md":{"filePath":"202210151505.md","links":[{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210151509","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210151447","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210150928","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210101627","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"A form of hypothesis testing. This uses the same statistic as the t-test, but the interpretation is different, and as a result the distribution of the test statistic also differs:"},"tags":[]},"parentNote":"index.md","title":"Wald test","url":"202210151505.html"},"202210151509.md":{"filePath":"202210151509.md","links":[{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Averages of large samples approach Gaussian random variables."},"tags":[]},"parentNote":"index.md","title":"Central limit theorem","url":"202210151509.html"},"202210151513.md":{"filePath":"202210151513.md","links":[{"resolvedRelTarget":{"contents":"202210151517.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151517.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091450.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091450.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071208.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071208.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"We wish to test whether two discrete variables are independent. The chi-square test would be an appropriate test, for every combination of the two variables we may compare the frequency of co-occurrence against the product of the marginal frequencies."},"tags":[]},"parentNote":"index.md","title":"Hypothesis testing independence","url":"202210151513.html"},"202210151517.md":{"filePath":"202210151517.md","links":[{"resolvedRelTarget":{"contents":"202210150928.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210150928.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A type of hypothesis testing, where we compare the expected vs the observed outcomes."},"tags":[]},"parentNote":"index.md","title":"Chi-square test","url":"202210151517.html"},"202210151520.md":{"filePath":"202210151520.md","links":[{"resolvedRelTarget":{"contents":"202210150928.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210150928.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A type of hypothesis testing, where measurements for a single unit (e.g. a certain person, a certain bottle of beer) are compared: $$"},"tags":[]},"parentNote":"index.md","title":"Paired test","url":"202210151520.html"},"202210151638.md":{"filePath":"202210151638.md","links":[],"meta":{"page":{"description":"STAT0032"},"tags":[]},"parentNote":"index.md","title":"Questions","url":"202210151638.html"},"202210170939.md":{"filePath":"202210170939.md","links":[{"resolvedRelTarget":{"contents":"202210170946.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210170946.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141516.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141516.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141045.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141045.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"An ensemble of binary classification models but instead of using 0 and 1 to classify, we use -1 and 1. AdaBoost also promotes the use of weak learners (binary models with accuracy over 50%, i.e. slightly better than random)."},"tags":[]},"parentNote":"index.md","title":"AdaBoost","url":"202210170939.html"},"202210170946.md":{"filePath":"202210170946.md","links":[{"resolvedRelTarget":{"contents":"202210141453.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141453.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A decision tree with only 1 split."},"tags":[]},"parentNote":"index.md","title":"Decision stump","url":"202210170946.html"},"202210171323.md":{"filePath":"202210171323.md","links":[{"resolvedRelTarget":{"contents":"202210141123.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141123.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210121758.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210121758.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210111043.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111043.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101339.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101339.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081016.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081016.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This type of model concerns modelling the posterior P(\\bold{w}|\\bold{x})"},"tags":[]},"parentNote":"index.md","title":"Discriminative model","url":"202210171323.html"},"202210171325.md":{"filePath":"202210171325.md","links":[{"resolvedRelTarget":{"contents":"202210111029.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210111029.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210090920.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210090920.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210090920.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210090920.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This type of model concerns modelling the likelihood P(\\bold{x}|\\bold{w}) (or P(\\bold{x}, \\bold{w}))."},"tags":[]},"parentNote":"index.md","title":"Generative model","url":"202210171325.html"},"202210171611.md":{"filePath":"202210171611.md","links":[{"resolvedRelTarget":{"contents":"202210151146.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151146.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151102.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151102.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"We follow the steps in Hypothesis testing procedure."},"tags":[]},"parentNote":"index.md","title":"STAT0032 Exercise Sheet 2","url":"202210171611.html"},"202210181455.md":{"filePath":"202210181455.md","links":[{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210061140","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"This allows one to fit the parameters \\theta of a models using paired training examples {\\bold{x}_i, \\bold{w}_i}."},"tags":[]},"parentNote":"index.md","title":"Learning algorithm","url":"202210181455.html"},"202210181456.md":{"filePath":"202210181456.md","links":[{"resolvedRelTarget":{"contents":"202210121758.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210121758.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210061140.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210061140.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This takes a new observation \\bold{x} and uses the associated model to return the posterior Pr(\\bold{w}|\\bold{x}, \\boldsymbol{\\theta})."},"tags":[]},"parentNote":"index.md","title":"Inference algorithm","url":"202210181456.html"},"202210190852.md":{"filePath":"202210190852.md","links":[{"resolvedRelTarget":{"contents":"202210250932.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250932.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091450.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091450.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A hidden variable \\bold{h}_i is associated with each data point \\bold{x}_i and induces the more complex properties of the resulting pdf."},"tags":[]},"parentNote":"index.md","title":"Hidden (latent) variables","url":"202210190852.html"},"202210191454.md":{"filePath":"202210191454.md","links":[],"meta":{"page":{"description":"A graph consists of nodes (vertices) and undirected or directed links (edges) between nodes."},"tags":[]},"parentNote":"index.md","title":"Graph","url":"202210191454.html"},"202210191610.md":{"filePath":"202210191610.md","links":[{"resolvedRelTarget":{"contents":"202210131227.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210131227.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210131116.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210131116.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071218.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071218.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210071208.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210071208.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Consider the following belief network:"},"tags":[]},"parentNote":"index.md","title":"Independence in belief networks","url":"202210191610.html"},"202210191728.md":{"filePath":"202210191728.md","links":[],"meta":{"page":{"description":"\\mathcal{X} and \\mathcal{Y} are d-connected by \\mathcal{Z} if there is any path from \\mathcal{X} to \\mathcal{Y} that is not blocked by \\mathcal{Z}. If \\mathcal{Z} is the empty set then \\mathcal{X} and \\mathcal{Y} are d-connected."},"tags":[]},"parentNote":"index.md","title":"d-connected/separated","url":"202210191728.html"},"202210191738.md":{"filePath":"202210191738.md","links":[{"resolvedRelTarget":{"contents":"202210191728.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210191728.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Consider the distribution (A|B)P(B|C)P(C):"},"tags":[]},"parentNote":"index.md","title":"Graphical dependence","url":"202210191738.html"},"202210191745.md":{"filePath":"202210191745.md","links":[{"resolvedRelTarget":{"contents":"202210191454.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210191454.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Formed from a graph by removing the arrows (removing the directedness)."},"tags":[]},"parentNote":"index.md","title":"Graph skeleton","url":"202210191745.html"},"202210191746.md":{"filePath":"202210191746.md","links":[],"meta":{"page":{"description":"An immorality in a DAG is a configuration of three nodes, A, B, C such that C is child of both A and B, with A and B not connected."},"tags":[]},"parentNote":"index.md","title":"Graph immorality","url":"202210191746.html"},"202210191748.md":{"filePath":"202210191748.md","links":[{"resolvedRelTarget":{"contents":"202210191746.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210191746.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210191745.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210191745.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210191454.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210191454.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Two graphs represent the same set of independence assumptions if and only if they have the same skeleton and the same set of immoralities."},"tags":[]},"parentNote":"index.md","title":"Markov equivalence","url":"202210191748.html"},"202210191857.md":{"filePath":"202210191857.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"numerical","url":"202210191857.html"},"202210191900.md":{"filePath":"202210191900.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"sentence","url":"202210191900.html"},"202210200938.md":{"filePath":"202210200938.md","links":[{"resolvedRelTarget":{"contents":"202210251108.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210251108.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250956.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250956.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250945.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250945.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250932.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250932.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Similar to the normal distribution, has a mean and a variance but also has a \\nu (degrees of freedom), which affects the shape of the curve. Generally speaking, student t-distributions are more accommodating of outliers."},"tags":[]},"parentNote":"index.md","title":"Student t-distribution","url":"202210200938.html"},"202210201118.md":{"filePath":"202210201118.md","links":[{"resolvedRelTarget":{"contents":"202210191454.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210191454.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210131111.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210131111.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081156.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081156.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"An undirected graph in which there is a potential (non-negative function) \\psi defined on each maximal clique."},"tags":[]},"parentNote":"index.md","title":"Markov networks","url":"202210201118.html"},"202210201151.md":{"filePath":"202210201151.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Iterated conditional modes (ICM)","url":"202210201151.html"},"202210201219.md":{"filePath":"202210201219.md","links":[],"meta":{"page":{"description":"Is B \\perp C|A, D? P(B|A,D,C) = P(B|A,D)?"},"tags":[]},"parentNote":"index.md","title":"Independence properties of Markov networks","url":"202210201219.html"},"202210201238.md":{"filePath":"202210201238.md","links":[{"resolvedRelTarget":{"contents":"202210081156.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081156.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A square node represents a factor (non negative function) of its neighbouring variables."},"tags":[]},"parentNote":"index.md","title":"Factor graphs","url":"202210201238.html"},"202210201346.md":{"filePath":"202210201346.md","links":[{"resolvedRelTarget":{"contents":"202210201423.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210201423.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210201406.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210201406.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210201358.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210201358.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210200938.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210200938.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151509.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151509.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101627.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101627.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081517.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081517.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Often, parameters of a population are estimated based on a sample. E.g. the average height of people in America might be estimated from the average height from a sample of 200 people."},"tags":[]},"parentNote":"index.md","title":"Confidence intervals","url":"202210201346.html"},"202210201358.md":{"filePath":"202210201358.md","links":[],"meta":{"page":{"description":"Denoted \\overline{X}, it is a random variable since it is a function of the data"},"tags":[]},"parentNote":"index.md","title":"Sample mean","url":"202210201358.html"},"202210201406.md":{"filePath":"202210201406.md","links":[{"resolvedRelTarget":{"contents":"202210101627.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101627.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A function of the parameter of interest which has a known distribution."},"tags":[]},"parentNote":"index.md","title":"Pivot","url":"202210201406.html"},"202210201423.md":{"filePath":"202210201423.md","links":[],"meta":{"page":{"description":"It’s a measure of randomness in the data, not randomness in the estimator."},"tags":[]},"parentNote":"index.md","title":"Coverage","url":"202210201423.html"},"202210201519.md":{"filePath":"202210201519.md","links":[{"resolvedRelTarget":{"contents":"202210201523.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210201523.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210201346.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210201346.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210201346.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210201346.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210151509.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210151509.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101627.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101627.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210081517.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210081517.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Assuming that our data is representative of a population, we consider that the sample we have taken is the whole population. We can then employ the methods listed in confidence intervals to take samples from our samples. One was to achieve this is with sampling with replacement. This is used to generate re-samples of the same size as the original sample."},"tags":[]},"parentNote":"index.md","title":"Bootstrap","url":"202210201519.html"},"202210201523.md":{"filePath":"202210201523.md","links":[{"resolvedRelTarget":{"contents":"202210201519.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210201519.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Used in bootstrapping, we randomly select a data point in the sample, add it to the re-sample, and then put it back in the box."},"tags":[]},"parentNote":"index.md","title":"Sampling with replacement","url":"202210201523.html"},"202210201612.md":{"filePath":"202210201612.md","links":[{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210201626","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210201406","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}},{"resolvedRelTarget":{"tag":"RRTMissing"},"unresolvedRelTarget":{"contents":{"contents":"202210201346","tag":"ModelRoute_StaticFile"},"tag":"URTResource"}}],"meta":{"page":{"description":"Here we use the bootstrap to estimate the distribution of \\hat{\\theta}_n -\n\\theta, which will be our pivot. Here, \\hat{\\theta}_n is our estimator for a certain parameter (could the sample mean) and \\theta is the parameter for which we want to create an interval."},"tags":[]},"parentNote":"index.md","title":"Bootstrap pivotal interval","url":"202210201612.html"},"202210201626.md":{"filePath":"202210201626.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Cumulative distribution function","url":"202210201626.html"},"202210201712.md":{"filePath":"202210201712.md","links":[],"meta":{"page":{"description":"p(a,b) \\ge p(a) + p(b) - 1"},"tags":[]},"parentNote":"index.md","title":"Bonferroni inequality","url":"202210201712.html"},"202210201718.md":{"filePath":"202210201718.md","links":[],"meta":{"page":{"description":"$$ n_{i,j}^k = \\sum_{l} \\underbrace{A_{ij}}{\\text{1 timestep}}\\overbrace{n{lj}^{k"},"tags":[]},"parentNote":"index.md","title":"Barber Book Exercises","url":"202210201718.html"},"202210201719.md":{"filePath":"202210201719.md","links":[{"resolvedRelTarget":{"contents":"202210191454.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210191454.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"An adjacency matrix A represents a graph where the value of A_{ij} is either 1 or 0 depending on whether there is an edge from vertex i to j."},"tags":[]},"parentNote":"index.md","title":"Adjacency matrix","url":"202210201719.html"},"202210201820.md":{"filePath":"202210201820.md","links":[],"meta":{"page":{"description":"For timeseries data v_1, \\ldots, v_t we need to model P(v_{1:T}). Consider the decomposition"},"tags":[]},"parentNote":"index.md","title":"Markov model","url":"202210201820.html"},"202210201823.md":{"filePath":"202210201823.md","links":[],"meta":{"page":{"description":"Only the recent past is relevant:"},"tags":[]},"parentNote":"index.md","title":"Markov chain","url":"202210201823.html"},"202210210859.md":{"filePath":"202210210859.md","links":[{"resolvedRelTarget":{"contents":"202210210913.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210210913.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210141045.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210141045.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"A linear binary classification model. The algorithm looks like so:"},"tags":[]},"parentNote":"index.md","title":"Perceptron","url":"202210210859.html"},"202210210913.md":{"filePath":"202210210913.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Linearly separability","url":"202210210913.html"},"202210210932.md":{"filePath":"202210210932.md","links":[{"resolvedRelTarget":{"contents":"202210211034.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210211034.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210211002.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210211002.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210210946.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210210946.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210210913.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210210913.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210170939.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210170939.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Good for linearly separable data where we are looking to create a hyperplane with a threshold at 0. Similar to AdaBoost, we use a y \\in \\left\\{ 1, -1 \\right\\} class system (rather than \\left\\{ 1, 0 \\right\\}). The intercept term is also separated out as a separate component, rather than being incorporated into the model as a dummy feature in the weights vector at w_0."},"tags":[]},"parentNote":"index.md","title":"Optimal hard margin","url":"202210210932.html"},"202210210933.md":{"filePath":"202210210933.md","links":[{"resolvedRelTarget":{"contents":"202210211114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210211114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210211035.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210211035.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"These are the optimal soft margin classifiers used with an optional kernel trick."},"tags":[]},"parentNote":"index.md","title":"Support vector machines","url":"202210210933.html"},"202210210946.md":{"filePath":"202210210946.md","links":[],"meta":{"page":{"description":"The distance from the training data to a hyperplane separating the classifications of the training data."},"tags":[]},"parentNote":"index.md","title":"Margin","url":"202210210946.html"},"202210211002.md":{"filePath":"202210211002.md","links":[],"meta":{"page":{"description":"We want to maximise f(x) subject to: g(x) = 0:"},"tags":[]},"parentNote":"index.md","title":"Lagrange multipliers","url":"202210211002.html"},"202210211034.md":{"filePath":"202210211034.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Support vectors","url":"202210211034.html"},"202210211035.md":{"filePath":"202210211035.md","links":[{"resolvedRelTarget":{"contents":"202210210932.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210210932.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210210913.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210210913.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Unlike optimal hard margin which needs linearly separable data, soft margin allows us to fit non-linearly separable data, i.e. we want to allow some of the classifications to be wrong."},"tags":[]},"parentNote":"index.md","title":"Soft margin","url":"202210211035.html"},"202210211114.md":{"filePath":"202210211114.md","links":[],"meta":{"page":{"description":"\nK(u, v) = (1 + \\bold{u} \\cdot \\bold{v})^p\n"},"tags":[]},"parentNote":"index.md","title":"Kernel trick","url":"202210211114.html"},"202210241230.md":{"filePath":"202210241230.md","links":[{"resolvedRelTarget":{"contents":"202210241300.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210241300.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210171323.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210171323.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091114.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091114.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"For a discriminative model, we choose a normal distribution and assume there’s a linear relation between our inputs \\bold{x}_i and the mean of our distribution:"},"tags":[]},"parentNote":"index.md","title":"Linear regression","url":"202210241230.html"},"202210241300.md":{"filePath":"202210241300.md","links":[{"resolvedRelTarget":{"contents":"202210241230.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210241230.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210121758.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210121758.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210090920.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210090920.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"We compute the probability distribution over possible values of the parameters \\boldsymbol{\\phi}. When we evaluate the probability of new data, we take a weighted average of the predictions induced by the different possible values."},"tags":[]},"parentNote":"index.md","title":"Bayesian regression","url":"202210241300.html"},"202210241625.md":{"filePath":"202210241625.md","links":[],"meta":{"page":{"description":"Make sure you know your audience. Shouldn’t assume very advanced statistical knowledge."},"tags":[]},"parentNote":"index.md","title":"STAT0032 Group Project","url":"202210241625.html"},"202210241714.md":{"filePath":"202210241714.md","links":[],"meta":{"page":{"description":"\n\\begin{array}{c}\n\\text{subjects} & \\text{placebo} & \\text{old} & \\text{new} \\\\\n1 & \\ldots & \\ldots & \\ldots \\\\\n\\vdots \\\\\n8\n\\end{array}\n"},"tags":[]},"parentNote":"index.md","title":"STAT0032 Exercise Sheet 3","url":"202210241714.html"},"202210250932.md":{"filePath":"202210250932.md","links":[{"resolvedRelTarget":{"contents":"202210250946.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250946.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250945.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250945.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101339.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101339.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The EM algorithm is used to find maximum likelihood or MAP estimates of model parameters \\boldsymbol{\\theta}. It is especially useful when there are no closed form solutions for ML and MAP."},"tags":[]},"parentNote":"index.md","title":"Expectation maximization","url":"202210250932.html"},"202210250945.md":{"filePath":"202210250945.md","links":[{"resolvedRelTarget":{"contents":"202210090920.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210090920.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"In the E-step at iteration t+1 we set each distribution q_i(\\bold{h}_i) to be the posterior distributions P(\\bold{h}_i|\\bold{x}_i, \\boldsymbol{\\theta}) over the hidden variable given the associated data example and the current parameters \\boldsymbol{\\theta}^{[t]}. Using Bayes’ rule:"},"tags":[]},"parentNote":"index.md","title":"E-step","url":"202210250945.html"},"202210250946.md":{"filePath":"202210250946.md","links":[{"resolvedRelTarget":{"contents":"202210250932.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250932.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"In the M-step, we directly maximize the bound given in Expectation maximization with respect to the parameters \\boldsymbol{\\theta}."},"tags":[]},"parentNote":"index.md","title":"M-step","url":"202210250946.html"},"202210250956.md":{"filePath":"202210250956.md","links":[{"resolvedRelTarget":{"contents":"202210250946.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250946.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250945.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250945.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250932.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250932.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101307.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101307.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091049.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091049.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The data are described as a weighted sum of K normal distributions"},"tags":[]},"parentNote":"index.md","title":"Mixture of Gaussians (MoG)","url":"202210250956.html"},"202210251108.md":{"filePath":"202210251108.md","links":[],"meta":{"page":{"description":"A continuous probability distribution defined on the positive real axis with probability density function:"},"tags":[]},"parentNote":"index.md","title":"Gamma distribution","url":"202210251108.html"},"202210251137.md":{"filePath":"202210251137.md","links":[{"resolvedRelTarget":{"contents":"202210251144.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210251144.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250932.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250932.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210130950.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210130950.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101307.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101307.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210091450.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210091450.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"Often with visual data we are dealing with very large dimensions of data. To model this with an multivariate normal distribution for example, would require a very large covariance matrix which has its implications in memory and in computing the inverse for the normal likelihood. One could use a diagonal matrix for the covariance but this assumes that each dimension of the data is independent (unlikely for an image of a face as neighbouring pixels in the same region of the face would covary very closely)."},"tags":[]},"parentNote":"index.md","title":"Factor analysis","url":"202210251137.html"},"202210251144.md":{"filePath":"202210251144.md","links":[],"meta":{"tags":[]},"parentNote":"index.md","title":"Linear subspaces","url":"202210251144.html"},"202210251205.md":{"filePath":"202210251205.md","links":[{"resolvedRelTarget":{"contents":"202210251137.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210251137.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210250956.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210250956.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210200938.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210200938.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"This is combination of MoG, t-distributions, and factor analyzers with an associated density function:"},"tags":[]},"parentNote":"index.md","title":"Mixture of robust subspace models","url":"202210251205.html"},"202210251254.md":{"filePath":"202210251254.md","links":[],"meta":{"page":{"description":"A formula that allows us to reduce the number of dimensions in a matrix to be inverted. It still goes through inversion but the number of dimensions can be significantly reduced."},"tags":[]},"parentNote":"index.md","title":"Matrix inversion lemma","url":"202210251254.html"},"202210251356.md":{"filePath":"202210251356.md","links":[{"resolvedRelTarget":{"contents":"202210241230.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210241230.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}},{"resolvedRelTarget":{"contents":"202210101331.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"contents":{"contents":"202210101331.md","tag":"LMLRoute_Md"},"tag":"ModelRoute_LML"},"tag":"URTResource"}}],"meta":{"page":{"description":"The key idea is that you make a non-linear function from a linear weighted sum of non-linear basis functions."},"tags":[]},"parentNote":"index.md","title":"Non-linear regression","url":"202210251356.html"},"index.md":{"filePath":"index.md","links":[{"resolvedRelTarget":{"contents":"-/all.html","tag":"RRTFound"},"unresolvedRelTarget":{"contents":{"tag":"VirtualRoute_Index"},"tag":"URTVirtual"}}],"meta":{"page":{"description":"Welcome to Emanote."},"tags":["emanote/default-layer"]},"parentNote":null,"title":"My Emanote Site","url":""}},"version":1}