# Combinatorial search

A process of selecting a suitable model. Consists of going through every model
possible and comparing their performance.

Core idea: given the original set of $p$ predictors, propose several subsets of
it, then score how good/bad they are. Note that comparing models of different
sizes requires careful consideration.

### Best subset selection algorithm

1. Let $\mathcal{M}_0$ denote the *null model*, which contains no predictors
   (just the intercept).
2. For $k = 1, 2, \ldots, p$
    * Fit all $\binom{p}{k}$ models that contain exactly $k$ predictors
    * Pick the best among these $\binom{p}{k}$ models, and call it
    $\mathcal{M}_k$. Here *best* is defined as having the smallest 
    [residual sum of squares](202210111531.md), or equivalently the largest $R^2$
3. Select a single best model from among the $\mathcal{M}_0, \ldots, \mathcal{M}_{p}$
   using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.

For the scoring, we will follow this general idea:

$$
Score = -fitness + complexity \text{  } penalty
$$

How to pick a criterion for subset selection? If main goal is prediction,
something like [cross validation](202210111811.md) is a good choice (although is
expensive and more unstable). If focus is on model fitness (as given by
likelihood) then an information criterion, e.g. [AIC](202211171425.md) can be a
good option.
