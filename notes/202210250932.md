# Expectation maximization

The EM algorithm is used to find [maximum likelihood](202210101331) or
[MAP](202210101339) estimates of model parameters $\boldsymbol{\theta}$. It is
especially useful when there are no closed form solutions for ML and MAP.

$$
\begin{equation}
\hat{\boldsymbol{\theta}} = \argmax_{\boldsymbol{\theta}} \left[
\sum_{i=1}^{I} \log \left[
\int P(\bold{x}_i, \bold{h}_i|\boldsymbol{\theta}) d\bold{h}_i
\right]
\right]
\end{equation}
$$

It works by defining a lower bound $\mathcal{B}[\{q_i(\bold{h}_i)\}, \boldsymbol{\theta}]$
on $(1)$ and iteratively increasing this bound. The lower bound chosen is

$$
\begin{equation}
\mathcal{B}[\{q_i(\bold{h}_i)\}, \boldsymbol{\theta}] =
\sum_{i=1}^{I} \int q_i(\bold{h}_i) \log \left[
\frac{P(\bold{x}_i, \bold{h}_i|\theta)}{q_i(\bold{h}_i)}
\right] d\bold{h}_i
\end{equation}
$$

Here, $\{q_i(\bold{h}_i)\}_{i=1}^I$ are the set of $I$ probability distributions
over the hidden variables $\{\bold{h}_i\}_{i=1}^I$.

The EM algorithm manipulates both $\boldsymbol{\theta}$ and the distributions
$\{q_i(\bold{h}_i)\}_{i=1}^I$ to increase the lower bound. It alternates
between:

* updating the probability distributions $\{q_i(\bold{h}_i)\}_{i=1}^I$ to
  improve the bound in $(2)$. This is known as the *expectation step* or
  [E-step](202210250945).
* updating the parameters $\boldsymbol{\theta}$ to improve the bound in $(2)$.
  This is known as the maximization step or [M-step](202210250946).

The E- and M-steps are alternated until the bound on the data no longer
increases and the parameters no longer change.
