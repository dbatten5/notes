# Soft margin

Unlike [optimal hard margin](202210210932) which needs [linearly
separable](202210210913) data, soft margin allows us to fit non-linearly
separable data, i.e. we want to allow some of the classifications to be wrong.

One way to do this is to introduce *slack variables* into the optimisation.
These allow a certain amount of leeway into which margin is chosen. One of these
slack variables $\xi$ exists for each training sample - it's a measure of how
much slack has been taken up by that example. We impose a limit to the total
amount slack $\sum_{i} \xi_i \le c$:

$$
\text{maximise}_{\bold{w}, b, \xi} \hspace{0.5em} M \\[0.5em]
\text{subject to:} \\[0.5em]
\hspace{1em} y_i(\bold{x}_i \cdot \bold{w} + b) \ge M(1 - \xi_i) \\[0.5em]
\|\bold{w}\| = 1 \\[0.5em]
\xi_i \ge 0 \hspace{1em}  \forall i \\[0.5em]
\sum_{i} \xi_i \le \text{constant}
$$

Gives us:

$$
\text{minimise}_{\bold{w}, b, \xi} \hspace{0.5em} \frac{1}{2} \|\bold{w}\|^2 + C \sum_{i}^{n} \xi_i \\[0.5em]
\text{subject to:} \\[0.5em]
\hspace{1em} y_i(\bold{x}_i \cdot \bold{w} + b) \ge 1 - \xi_i \\[0.5em]
\xi_i \ge 0 \hspace{1em}
$$

