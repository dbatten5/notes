# Ordinary least squares

This is a method for fitting a linear model. To solve it we solve the [Residual sum of squares](202210111531), i.e. we solve
$$
\bold{w}^* = \argmin_{\bold{w}} \|\bold{X}\bold{w} - \bold{y}\|^2
$$

In order to do this, we will be taking the derivative with respect to the vector
$\bold{w}$. To do this, we'll be taking the partial derivatives with respect to
each of the elements of $\bold{w}$, ending up with a vector of partial
derivatives, known as a [gradient](202210111543): 
$$
\nabla_{\bold{w}}L = \begin{bmatrix} 
\frac{\partial L}{\partial \omega_1} \\[0.5em]
\frac{\partial L}{\partial \omega_2} \\
\vdots \\
\frac{\partial L}{\partial \omega_m}
\end{bmatrix}
$$

### Solving

Expanding out $\|\bold{X}\bold{w} - \bold{y}\|^2$ and simplifying
using properties of [matrix multiplication](202210111544) gives us: 
$$
\begin{align*}
\|\bold{X}\bold{w} - \bold{y}\|^2 &= (\bold{X}\bold{w} - \bold{y)^{\top}(\bold{X}\bold{w} - \bold{y})} \\
&= \enspace  \enspace \vdots \\[0.5em]
&= \bold{w}^{\top}(\bold{X}^{\top}\bold{X})\bold{w} - 2(\bold{X}^{\top}\bold{y})^{\top}\bold{w} + \bold{y}^{\top}\bold{y}
\end{align*}
$$

Then taking gradient of the function gives us:
$$
\begin{align*}
\nabla_{\bold{w}}L &= \nabla_{\bold{w}}(\bold{w}^{\top}(\bold{X}^{\top}\bold{X})\bold{w} - 2(\bold{X}^{\top}\bold{y})^{\top}\bold{w} + \bold{y}^{\top}\bold{y}) \\[0.5em]
&= 2\bold{X}^{\top}\bold{X}\bold{w} - 2\bold{X}^{\top}\bold{y} \\[0.5em]
&= 0 \\[0.5em]
\bold{X}^{\top}\bold{X}\bold{w}^* &= \bold{X}^{\top}\bold{y} \\[0.5em]
\bold{w}^* &= (\bold{X}^{\top}\bold{X})^{-1}\bold{X}^{\top}\bold{y}
\end{align*}
$$

Note that this assumes that $\bold{X}^{\top}\bold{X}$ is invertible.
