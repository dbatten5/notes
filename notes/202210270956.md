# Sparse linear regression

Perhaps not every dimension of the data $\bold{x}$ is informative. A sparse
solution forces some of the coefficients in $\boldsymbol{\phi}$ to be zero.

Recall from [linear regression](202210241230.md) that $\boldsymbol{\phi}$ is a
gradient vector, and there is a $\phi_i$ for every dimension of the data.

Method:
* apply a different prior on $\boldsymbol{\phi}$ that encourages sparsity.
* product of [t-distributions](202210200938.md).

$$
\begin{align*}
P(\boldsymbol{\phi}) &= \prod_{d=1}^{D} \text{Stud}_{\phi_d}\left[ 0, 1, \nu \right]  \\[0.5em]
&= \prod_{d=1}^{D} \frac{\Gamma \left( \frac{\nu + 1}{2} \right) }
{\sqrt{ \nu \pi } \Gamma \left( \frac{\nu}{2} \right) }
\left( 1 + \frac{\phi^2_d}{\nu} \right)^{-(\nu + 1)/2}
\\[0.5em]
\end{align*}
$$

Where $D$ is the number of dimensions. We can use

$$
P(\boldsymbol{\phi}| \bold{X}, \bold{w}, \sigma^2) =
\frac{P(\bold{w}|\bold{X}, \boldsymbol{\phi}, \sigma^2)P(\boldsymbol{\phi})}
{P(\bold{w}|\bold{X}, \sigma^2)}
$$

However, now the prior is not conjugate to the normal likelihood, so we cannot
compute the posterior in closed form.

$$
P(\bold{w|\bold{X}, \sigma^2}) \approx
\max_{\bold{H}} \left[
\text{Norm}_{\bold{w}}\left[
\bold{0}, \bold{X}^{\top}\bold{H}^{-1}\bold{X} + \sigma^2\bold{I}
\right]
\prod_{d=1}^{D} \text{Gam}_{h_d} \left[ \nu / 2, \nu / 2 \right]
\right]
$$

Here, $\bold{H}$ is a diagonal matrix with hidden variables $\left\{ h_d \right\}$ on the diagonal.

To fit the model, update the variance $\sigma^2$ and hidden variables $\left\{ h_d \right\}$

$$
h_{d}^{new} = \frac{1 - h_d \Sigma_{dd} + \nu}{\mu^2_d + \nu}
$$

Where $\mu_d$ is the $d^{th}$ element of the mean $\boldsymbol{\mu}$ of the
posterior over the weights $\boldsymbol{\phi}$ and $\Sigma_{dd}$ is the $d^{th}$
element of the of the diagonal of the covariance $\boldsymbol{\Sigma}$ of the
posterior over the weights with

$$
\begin{align*}
\boldsymbol{\mu} &= \frac{1}{\sigma^2}\bold{A}^{-1}\bold{X}\bold{w} \\[0.5em]
\Sigma &= \bold{A}^{-1} \\[0.5em]
\bold{A} &= \frac{1}{\sigma^2}\bold{X}\bold{X}^{\top} + \bold{H} \\[0.5em]
\end{align*}
$$

To choose variance

$$
(\sigma^2)^{new} = \frac{1}{D - \Sigma_d(1 - h_d \Sigma_{dd})}
(\bold{w} - \bold{X}\boldsymbol{\mu})^{\top}(\bold{w} - \bold{X}\boldsymbol{\mu})
$$

Typically a very small value for degrees of freedom $\nu < 10^{-3}$ is chosen to
encourage sparseness. At the end of training, all dimensions of $\boldsymbol{\phi}$
where the hidden variable $h_d$ is large (e.g. $>1000$) are discarded.
