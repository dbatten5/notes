# Residual sum of squares

Convenient to consider the Euclidean
distance between the observed output vs the predicted output as a form of loss
function, $\|\bold{y} - \bold{\hat{y}}\|^2$. This is known as the Euclidean, or
L2, norm. To get our loss function, we sum the norms as (using the notation from
[linear models](2022101114)):
$$
\sum_{n=1}^{n} \|y_i - \hat{y}_i\|^2 = \sum_{n=1}^{n} \|y_i - \bold{x}_i \cdot
\bold{w}\|^2 = \|\bold{X}\bold{w} - \bold{y}\|^2
$$
To solve it, we solve
$$
\bold{w}^* = \argmin_{\bold{w}} \|\bold{X}\bold{w} - \bold{y}\|^2
$$

In order to do this, we will be taking the derivative with respect to the vector
$\bold{w}$. To do this, we'll be taking the partial derivatives with respect to
each of the elements of $\bold{w}$, ending up with a vector of partial
derivatives, known as a [gradient](202210111543): 
$$
\nabla_{\bold{w}}L = \begin{bmatrix} 
\frac{\partial L}{\partial \omega_1} \\[0.5em]
\frac{\partial L}{\partial \omega_2} \\
\vdots \\
\frac{\partial L}{\partial \omega_m}
\end{bmatrix} 
$$

### Solving

Expanding out $\|\bold{X}\bold{w} - \bold{y}\|^2$ and simplifying
using properties of [matrix multiplication](202210111544) gives us: 
$$
\begin{align*}
\|\bold{X}\bold{w} - \bold{y}\|^2 &= (\bold{X}\bold{w} - \bold{y)^{\top}(\bold{X}\bold{w} - \bold{y})} \\
&= \enspace  \enspace \vdots \\[0.5em]
&= \bold{w}^{\top}(\bold{X}^{\top}\bold{X})\bold{w} - 2(\bold{X}^{\top}\bold{y})^{\top}\bold{w} + \bold{y}^{\top}\bold{y}
\end{align*}
$$

Then taking gradient of the function gives us:
$$
\begin{align*}
\nabla_{\bold{w}}L &= \nabla_{\bold{w}}(\bold{w}^{\top}(\bold{X}^{\top}\bold{X})\bold{w} - 2(\bold{X}^{\top}\bold{y})^{\top}\bold{w} + \bold{y}^{\top}\bold{y}) \\[0.5em]
&= 2\bold{X}^{\top}\bold{X}\bold{w} - 2\bold{X}^{\top}\bold{y} \\[0.5em]
&= 0 \\[0.5em]
\bold{X}^{\top}\bold{X}\bold{w}^* &= \bold{X}^{\top}\bold{y} \\[0.5em]
\bold{w}^* &= (\bold{X}^{\top}\bold{X})^{-1}\bold{X}^{\top}\bold{y}
\end{align*}
$$

Note that this assumes that $\bold{X}^{\top}\bold{X}$ is invertible.
