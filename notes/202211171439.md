# Stepwise selection

In [combinatorial search](202211171401.md), common to use greedy search:
1. Start with an initial candidate model
2. Look at its "neighbours". If any candidate is better than the current
   candidate, make it the current candidate. Else, stop
3. Return to step 2.

A starting model can be the model with just the intercept term, or a model with
all the covariates. A "neighbour" is a model with the number of covariates
differing by 1. Adding (or removing, depending on the starting point) a
covariate to (or from) the current candidate is known as *stepwise selection*
and has the following algorithm for **forward stepwise selection**:

1. Let $\mathcal{M}_0$ denote the *null model*, which contains no predictors
   (just the intercept).
2. For $k = 1, 2, \ldots, p - 1$
    * Consider all $p - k$ models that augment the predictors in $\mathcal{M}_k$
    with one additional parameter.
    * Choose the *best* among these $p - k$ models, and call it
    $\mathcal{M}_{k+1}$. Here *best* is defined as having the smallest $RSS$ or
    highest $R^2$.
3. Select a single best model from among the $\mathcal{M}_0, \ldots, \mathcal{M}_{p}$
   using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.

**Backward stepwise selection** is the same as forward, but you start with $\mathcal{M}_p$ as a *full* model and then iterate through
$k = p, p - 1, \ldots, 1$ considering all $k$ models that contain 1 fewer
predictors in $\mathcal{M}_k$ for a total of $k - 1$ predictors. Then choose the
*best* among these $k$ models, and call it $\mathcal{M}_{k-1}$.
