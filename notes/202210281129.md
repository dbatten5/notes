# Backpropagation

This relates to calculating a gradient of loss function by passing information
back through a network.

The weights and biases are updated like so:

$$
w_{new} = w_{old} - \alpha * w_{old}
$$

Where $\alpha$ is the learning rate. The same goes for the bias terms.
