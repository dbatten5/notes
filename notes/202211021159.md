# Newton's gradient optimization

Also known as Newton-Raphson optimization.

A form of [gradient based optimization](202211011714.md) and similar to 
[steepest descent](202211021149.md) but considers both the gradient of the
function and how the gradient is changing (second derivate).

The motivation is that if the second derivative is low, then the gradient is
changing slowly and we can move larger distances in the line search. Conversely,
if the second derivative is large, then things are changing quickly and we
should only move a small distance.

$$
\boldsymbol{\theta}^{\left[ t + 1 \right]} = \boldsymbol{\theta}^{\left[ t \right]} - 
\lambda 
\left( \frac{\partial^2 f}{\partial \boldsymbol{\theta}^2} \right)^{-1}
\frac{\partial f}{\partial \boldsymbol{\theta}}
$$

With $\lambda$ set to one, or we can find the optimal value using line search.
