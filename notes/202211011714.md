# Gradient based optimization

This is a form of [iterative non-linear optimization](202210311318.md). It
follows this pattern:
* choose a search direction $\bold{s}$ based on the local properties of the
function
* search to find the minimum along the chosen direction. In other words, we seek
the distance $\lambda$ such that

$$
\hat{\lambda} = \argmin_{\lambda}\left[
f\left[
\boldsymbol{\theta}^{\left[ t \right]} + \lambda s
\right]
\right]
$$

Then set
$\boldsymbol{\theta}^{\left[ t + 1 \right]} = \boldsymbol{\theta}^{\left[ t \right]} + 
\hat{\lambda}\bold{s}
$. This is termed a [line search](202211021023.md).

## Choosing a search direction
