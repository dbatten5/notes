# Linear regression

For a [discriminative model](202210171323.md), we choose a [normal distribution](202210091114.md)
and assume there's a linear relation between our inputs $\bold{x}_i$ and the
mean of our distribution:

$$
P(w_i| \bold{x}_i, \boldsymbol{\theta}) = \text{Norm}_{w_i}\left[
\underbrace{\phi_0 + \boldsymbol{\phi}^{\top}\bold{x}_i}_{\text{linear}}, \sigma^2
\right]
$$

Here every $i$ would be a different input, e.g. a different image. Need to
estimate $\phi_0$ and $\phi_1$.

To make notation easier to handle,
* Attach a 1 to the start of every data vector:

$$
\bold{x}_i \leftarrow \begin{bmatrix} 1 & \bold{x}_i^{\top} \end{bmatrix} ^{\top}
$$

* Attach the offset to the start of the gradient vector $\boldsymbol{\phi}$

$$
\boldsymbol{\phi} \leftarrow \begin{bmatrix} \phi_0 & \boldsymbol{\phi}^{\top} \end{bmatrix} ^{\top}
$$

This gives a new model as

$$
P(w_i|\bold{x}_i, \boldsymbol{\theta}) = \text{Norm}_{w_i}\left[
\boldsymbol{\phi}^{\top} \bold{x}_i, \sigma^2
\right]
$$

The probability of the whole dataset can be written as

$$
P(\overbrace{\bold{w}}^{\text{[Ix1]}}|\overbrace{\bold{X}}^{\text{[DxI]}}, \boldsymbol{\theta}) =
\text{Norm}_{\bold{w}}\left[ \bold{X}^{\top}\boldsymbol{\phi},
\sigma^2 \bold{I} \right]
$$

Where

$$
\bold{X} = \left[ \bold{x}_1, \bold{x}_2, \ldots \bold{x}_I \right]
\hspace{1em}
\bold{w} = \left[ w_1, w_2, \ldots, w_I \right]^{\top}
$$

### Learning

Using [maximum likelihood](202210101331.md) we can estimate $\hat{\boldsymbol{\theta}}$:

$$
\begin{align*}
\hat{\phi} &= (\bold{X}\bold{X}^{\top})^{-1}\bold{X}\bold{w} \\[0.5em]
\hat{\sigma}^2 &= \frac{(\bold{w} - \bold{X}^{\top}\boldsymbol{\phi})^{\top}(\bold{w} - \bold{X}^{\top}\boldsymbol{\phi})}
{\bold{I}} \\[0.5em]
\end{align*}
$$

This type of linear regression has a few problems:
1. It's **overconfident**, it has a high confidence in areas where there was
   little training data. This can be improved with [Bayesian regression](202210241300.md)
