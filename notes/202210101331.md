# Maximum likelihood

Find the parameters $\boldsymbol{\hat{\theta}}$ under which the data $\bold{x}_{1\ldots I}$ are most likely.

$$
\begin{align*}
\boldsymbol{\hat{\theta}} &= \argmax_{\theta}[Pr(\bold{x}_{1\ldots\bold{I}}|\boldsymbol{\theta})] \\
&= \argmax_{\theta}\left[\prod_{i=1}^{I} Pr(\bold{x}_i|\boldsymbol{\theta})\right] \\[0.5em]
\end{align*}
$$

In the equation, $I$ is the number of observations. Note that we are assuming
independence and can therefore use the product rule. The
$\boldsymbol{\hat{\theta}}$ are parameters that result in the best description
of our data. We can then evaluate a new data point $\bold{x}^*$ under the
probability distribution $Pr(\bold{x}^*|\boldsymbol{\hat{\theta}})$, with our
best parameters $\boldsymbol{\hat{\theta}}$.

E.g. if you were modelling a die (i.e. discrete variables), the $\theta$ would be the
probability of each of the values, a vector of the $\lambda_i$.

### Example: univariate normal

$$
\begin{align*}
  Pr(x_{1 \ldots I}|\mu, \sigma^2) &= \prod_{i=1}^{I} Pr(x_i|\mu, \sigma^2)  \\[0.5em]
  &= \prod_{i=1}^{I} \text{Norm}_{x_i}[ \mu, \sigma^2 ]  \\[0.5em]
  &= \frac{1}{(2\pi\sigma^2)^{I/2}}\text{exp}\left[ -0.5 \sum_{i=1}^{I} \frac{(x_i  - \mu)^2}{\sigma^2} \right] \\[0.5em]
\end{align*}
$$

For the log likelihood:

$$
\begin{align*}
Pr(x_{1 \ldots I}|\mu, \sigma^2) &=
-0.5 I \log[2 \pi] - 0.5 I \log \sigma^2 - 0.5 \sum_{i=1}^{I} \frac{(x_i -
\mu)^2}{\sigma^2}\\[0.5em]
\end{align*}
$$


The maximum likelihood will occur at the peak of this surface so that
$$
\hat{\mu}, \hat{\sigma}^2 = \argmax_{\mu, \sigma^2}[Pr(x_{1 \ldots I}|\mu, \sigma^2)]
$$

We maximise this by taking the derivative (of the log to make things a bit
easier) and setting this to 0.

This reduces down to:

$$
\begin{align*}
\hat{\mu} &= \frac{\sum_{i=1}^{I} x_i}{I} \\[0.5em]
\hat{\sigma}^2 &= \sum_{i=1}^{I} \frac{(x_i - \hat{\mu})^2}{I}
\end{align*}
$$

### 2 parameters with Gaussian noise

For $Y = \beta_0 + \beta_1 x_1 + \epsilon$, we can use ML to fit $\beta_0$ and
$\beta_1$. We have

$$
L(\beta_0, \beta_1, \sigma_{\epsilon}^2) =
\prod_{i=1}^{I} \frac{1}{\sqrt{2 \pi \sigma_{\epsilon}^2} }
\exp \left\{ -\frac{1}{2} \frac{(y^{(i)} - \beta_0 - \beta_1x_1^{(i)})^2}{\sigma_{\epsilon}^2} \right\}
$$

Working in the log-scale:

$$
\log L(\beta_0, \beta_1, \sigma_{\epsilon}^2) =
-0.5 \sum_{i=1}^{I} \left( \log(\sigma_{\epsilon}^2) +
\frac{(y^{(i)} - \beta_0 - \beta_1x_1^{(i)})^2}{\sigma_{\epsilon}^2}
\right)
$$

