# Kernel trick

This is often used in [non-linear regression](202210251356.md) problems when
it's costly to compute vectors $\bold{z}_i = \bold{f}\left[ \bold{x}_i \right]$
and $\bold{z}_j = \bold{f}\left[ \bold{x}_j \right]$ explicitly and then compute
the inner product $\bold{z}_i^{\top}\bold{z}_j$.

We can instead define a single *kernel function* $\bold{k}\left[ \bold{x}_i, \bold{x}_j \right]$
as a replacement for the operation 
$\bold{f}\left[ \bold{x}_i \right]^{\top} \bold{f}\left[ \bold{x}_j \right]$. It
is often more efficient to evaluate the kernel function directly.

### Polynomial kernel

$$
K(u, v) = (1 + \bold{u} \cdot \bold{v})^p
$$

### Radial basis (Gaussian) kernel

$$
K(u, v) = e^{-\gamma \|\bold{u} - \bold{v}\|^2}
= e^{- \frac{\|\bold{u} - \bold{v}\|^2}{2\sigma^2}} \\[0.5em]
\gamma = \frac{1}{2\sigma^2}
$$

Or in other words:

$$
K[\bold{x}_i, \bold{x}_j] = \exp \left[ -0.5\left(
\frac{(\bold{x}_i - \bold{x}_j)^{\top}(\bold{x}_i - \bold{x}_j)}{\lambda^2}
\right)  \right]
$$

Where $\lambda$ is the *length scale parameter*.

To estimate $\lambda$ (or $\sigma^2$), we can use [maximum likelihood](202210101331.md)

$$
P(\bold{w}|\bold{X}, \sigma^2) = \text{Norm}_{\bold{w}}\left[
  \bold{0},
  \sigma_p^2 \bold{K}[\bold{X}, \bold{X}] + \sigma^2 \bold{I}
\right] \\[0.5em]
\Rarr \hat{\lambda} = \argmax_{\lambda} \left[ 
\text{Norm}_{\bold{w}}\left[ 
  \bold{0},
  \sigma_p^2 \bold{K}[\bold{X}, \bold{X}] + \sigma^2 \bold{I}
\right]
\right]
$$

Here, $\bold{K}[\bold{X}, \bold{X}]$ represents a matrix of dot products where
element $(i, j)$ is given by $\text{k}[\bold{x}_i, \bold{x}_j]$ with $\text{k}$
being the kernel function.

