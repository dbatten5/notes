# Maximum a posteriori (MAP)

Here we introduce [prior](202210111029.md) information about the parameters $\boldsymbol{\theta}$. MAP maximizes the posterior probability $Pr(\boldsymbol{\theta}|\bold{x}_{1 \ldots I})$ of the parameters.

$$
\begin{align*}
\boldsymbol{\hat{\theta}} &= \argmax_{\theta}[Pr(\boldsymbol{\theta}|\bold{x}_{1\ldots I})] \\
&= \argmax_{\theta}\left[\prod_{i=1}^{I} Pr(\bold{x}_i|\boldsymbol{\theta})Pr(\boldsymbol{\theta})\right] \\[0.5em]
\end{align*}
$$

Here, Bayes' rule has been used to flip the condition, then the denominator can
be discarded since it doesn't depend on $\boldsymbol{\theta}$ and therefore
doesn't affect the maximum.

Comparing this to the [maximum likelihood](202210101331.md), we see that it is
identical with the exception of the additional prior term; ML is a special case
of MAP where the prior is uninformative.

This is often used when the training data set is "small". This is because we are
utilising some prior knowledge about the system, known as [prior](202210111029.md).

### Example: univariate normal

The maximum likelihood will occur at the peak of this surface so that 

$$
\begin{align*}
\hat{\mu}, \hat{\sigma}^2 &= \argmax_{\mu, \sigma^2} \left[ \prod_{i=1}^{I} Pr(x_i|\mu, \sigma^2)Pr(\mu,\sigma^2)  \right] \\[0.5em]
&= \argmax_{\mu, \sigma^2}\left[\prod_{i=1}^{I} \text{Norm}_{x_i}[\mu, \sigma^2] \text{NormInvGam}_{\mu, \sigma^2}[\alpha, \beta, \gamma, \delta]  \right] \\[0.5em]
\end{align*}
$$

Note that we have chosen the [Normal inverse gamma](202210091117.md) as the prior
as this is conjugate to the normal distribution (could have chosen something
else but this would make things harder). To find the $\hat{\mu}, \hat{\sigma}^2$
we substitute in the expressions, differentiate with respect to $\mu$ and
$\sigma$ and equate to zero to get expressions for $\hat{\mu}$ and $\hat{\sigma}^2$.
This gives us 
$$
\hat{\mu} = \frac{\sum_{i=1}^{I} x_i + \gamma\delta}{I + \gamma} \enspace
\enspace \text{and} \enspace \enspace \hat{\sigma}^2 = \frac{\sum_{i=1}^{I} (
x_i - \hat{\mu})^2 + 2\beta + \gamma(\delta - \hat{\mu})^2}
{I + 3 + 2\alpha}
$$

In other words 
$$
\hat{\mu} = \frac{I\overline{x} + \gamma\delta}{I + \gamma}
$$

This is a weighted sum of two terms. The data mean $\overline{x}$ is weighted by
the number of training samples $I$. The second term $\delta$, the value of
$\mu$ favoured by the prior, and is weighted by $\gamma$. This means that, with
large amounts of data, the first term dominates, and the MAP estimate is very
close to the ML estimate. With no data at all, the estimate is completely
governed by the prior.
