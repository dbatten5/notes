# Variational inference

Instead of dealing with a difficult distribution $p$, we are going to find the
best approximation to it in a class of simpler distributions $q$. Can use
[KL divergence](202211041003.md) to find "close" approximations. Since KL is not
symmetric, we need to choose either $KL(p||q)$ or $KL(q||p)$. Option 1 needs to
compute expectation under $p(x)$, which is hard. 

Option 2: $KL(q||p)$

$$
KL(q||p) = \sum q(x) \log \frac{p(x)}{q(x)} = \sum q(x)E(x) + \log(Z) - H(q)
$$

Want to find the best distribution $q$ within a certain family that maximises
Free Energy.

For [Markov net](202210201118.md) models with $p(x) = \frac{e^{-E(x)}}{Z}$

$$
F(q) = H(q) - \sum_{x} q(x)E(x) = \left< -E(x) \right>_{q(x)} + H(q)
$$

For the case of learning with hidden variables

$$
F(q) = \left< \log p(v,h|\theta) \right>_{q(h|\theta)} + H(q)
$$

