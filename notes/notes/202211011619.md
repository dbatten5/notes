# Relevance vector regression

This combines ideas of [dual regression](202211011453.md) (1 parameter per
training example) and [sparsity](202210270956.md) (most of the parameters are
zero). It can be thought of as a model that only depends sparsely on data.

This is achieved through imposing a penalty for every non-zero weighted training
example. This can be achieved by replacing the normal prior over the dual
parameters $\boldsymbol{\psi}$ with a product of 1 dimensional
[t-distributions](202210200938.md):

$$
P(\boldsymbol{\psi}) = \prod_{i=1}^{I} \text{Stud}_{\boldsymbol{\psi}_i}\left[
0, 1, \nu
\right]
$$

To solve for marginal likelihood $P(\bold{w}|\bold{X},\sigma^2)$, need to
approximate by updating variance $\sigma^2$ and hidden variables $h_i$
alternately:

$$
\begin{align*}
h_i^{new} &= \frac{1 - h_1 \Sigma_{ii} + \nu}{\mu^2_i + \nu} \\[0.5em]
(\sigma^2)^{new} &= \frac{1}{I - \sum_{i} (1 - h_i \Sigma_{ii})}
(\bold{w} - \bold{X}^{\top}\bold{X}\boldsymbol{\mu})^{\top}
(\bold{w} - \bold{X}^{\top}\bold{X}\boldsymbol{\mu})
\\[0.5em]
\end{align*}
$$

With

$$
\begin{align*}
\boldsymbol{\mu} &= \frac{1}{\sigma^2}\bold{A}^{-1}\bold{X}^{\top}\bold{X}\bold{w} \\[0.5em]
\boldsymbol{\Sigma} &= \bold{A}^{-1} \\[0.5em]
\bold{A} &= \frac{1}{\sigma^2}\bold{X}^{\top}\bold{X}\bold{X}^{\top}\bold{X} + \bold{H}
\end{align*}
$$

Again, data example where $h_i$ is large (e.g. $> 1000$) are discarded as their
$\boldsymbol{\psi}_i$ will be very small.
