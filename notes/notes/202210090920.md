# Bayes' rule

$$
\begin{align*}
P(y|x)P(x) &= P(x|y)P(y) \\[1em]
P(y|x) &= \frac{P(x|y)P(y)}{P(x)} \medspace \text{ or } \medspace \frac{P(x,y)}{P(x)}\\[1em]
&= \frac{P(x|y)P(y)}{\int P(x,y)dy} \\[1em]
&= \frac{P(x|y)P(y)}{\int P(x|y)P(y)dy} \\[1em]
\end{align*}
$$

### Conditioned version

$$
P(x|y,z) = \frac{P(y|x,z)P(x|z)}{P(y|z)}
$$

This comes from the fact that $P(x,y,z) = P(x|y,z)P(y|z)P(z)$.

Another useful property from Bayes'

$$
p(a,b|c) = p(a|b,c)p(b|c)
$$


## Terminology

- **Likelihood** - the propensity for observing a certain value of $x$ given a
  certain value of $y$.
- **Prior** - what we know about $y$ before seeing $x$.
- **[Evidence](202211081123.md)** - a constant to ensure that the left hand side is the valid
  distribution.
- **Posterior** - what we know about $y$ after seeing $x$.

$$
\underbrace{P(y|x)}_{\text{posterior}} = \frac{\overbrace{P(x|y)}^{\text{likelihood}}\overbrace{P(y)}^{\text{prior}}}
{\underbrace{\int P(x|y)P(y)dy}_{\text{evidence}}}
$$

With $y =$ some variable $\theta$, and $x =$ observed data $\mathcal{D}$ and given
a model $p(\mathcal{D}|\theta)$ and a prior belief $p(\theta)$ about which
variables are appropriate, we can infer a posterior $p(\theta|\mathcal{D})$,
meaning the probability of observing $\theta$ given the observed data
