# Discriminative model

This type of model concerns modelling the posterior $P(\bold{w}|\bold{x})$

How to model $P(\bold{w}|\bold{x})$?
1. Choose an appropriate form of $P(\bold{w})$
1. Make parameters a function of $\bold{x}$
1. Function takes parameters $\boldsymbol{\theta}$ that define its shape

Since the distribution over the state depends on both the data and these
parameters, we write it as $P(\bold{w}|\bold{x},\boldsymbol{\theta})$ and refer
to it as the [posterior distribution](202210121758.md).

**Learning algorithm**: learn parameters $\boldsymbol{\theta}$ from training
data $\bold{x}, \bold{w}$, which can be done by using the [ML](202210101331.md),
[MAP](202210101339.md), or [Bayesian approach](202210111043.md).

**Inference algorithm**: just evaluate $P(\bold{w}|\bold{x}, \boldsymbol{\theta})$ 
for new data $\bold{x}$.

### Regression

1. Choose normal distribution over $\bold{w}$
1. Make mean $\mu$ linear function of $x$ (variance constant) 

$$
P(w | x, \boldsymbol{\theta}) = \text{Norm}_{w} \left[ \phi_0 + \phi_1x, \sigma^2 \right]
$$

3. Parameters are $\phi_0, \phi_1, \sigma^2$

### Classification

We define a probability distribution over the world state $w \in \left\{ 0, 1
\right\}$ and make its parameters contingent on the data $x$. Since the world
state is discrete and binary, we use a [Bernoulli distribution](202210081016.md).
We make $\lambda$ a function of the data $x$ with $\phi_0 + \phi_1 x$, but we
need to ensure that the $0 \le \lambda \le 1$ is obeyed, so we pass the result
through the [sigmoid](202210141123.md) function 

$$
\tag{1.0} P(w|x) 
= \text{Bern}_w\left[ \text{sig}\left[ \phi_0 + \phi_1x \right]  \right] 
= \text{Bern}_w \left[ \frac{1}{1 + \exp\left[ -\phi_0 - \phi_1x \right] } \right] 
$$

This model is termed *logistic regression*, despite being used for
classification.

 In learning we fit $\boldsymbol{\theta} = \left\{ \phi_0, \phi_1 \right\}$ from
 $\bold{x}, \bold{w}$. In inference we substitute in the observed data value $x$
 into 1.0 to retrieve the posterior distribution $P(w|x)$.

If we had more than 2 world states, $w \in \left\{ 1 \ldots M \right\}$, we
would choose a [categorical distribution](202210091049.md):

$$
\begin{align*}
Pr(w = m|x) &= \text{Cat}_{w}\left[ \overline{\lambda}_{1 \ldots M} \right] \\[0.5em]
\overline{\lambda}_{1 \ldots M} &= \frac{\exp[y_m]}
{\sum_{k=1}^{M} \exp[y_k]}
\end{align*}
$$

Note this uses a softmax activation and the $\left\{ y_k \right\}$ are learned
values from the training data.
