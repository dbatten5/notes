# Kullback-Leibler divergence

KL divergence between two [pmfs](202210071722.md) "p" and "q" is defined as

$$
KL(p||q) \equiv \sum_{x} p(x) \log \frac{p(x)}{q(x)}
$$

Or

$$
KL(q(x) || p(x|\theta)) = \left<\log \frac{q(x)} {p(x|\theta)}\right>_{q(x)}
$$

For $x$ where $p(x) > 0$. It measures the "distance" between two pmfs.

Note that this is never negative, and equals zero if and only if $p(x) = q(x)$
for all $x$.

Also note that $KL(p||q) \ne KL(q||p)$ so in that sense $KL$ does not represent
a distance.

As an example, consider $p(x) = (\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4})$
and $q(x) = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3} - \epsilon, \epsilon)$

$KL$ will explode when $p(x)$ places mass were $q(x)$ doesn't. So here,
$KL(p||q) > KL(q||p)$ since $p(x)$ places mass $\frac{1}{4}$ where $q(x)$ is
$\epsilon$.

### Maximum likelihood

Let $q$ be the empirical distribution:

$$
q(x) = \frac{1}{N}\sum_{n=1}^{N} \mathbb{I}[x = x^n]
$$

Then

$$
\begin{align*}
KL(q||p(x|\theta)) &= \left<\log q(x)\right>_{q(x)} - \left<\log p(x|\theta)\right>_{q(x)} \\[0.5em]
&= -\frac{1}{N}\sum_{n=1}^{N} \log p(x^n|\theta) + \text{const} \\[0.5em]
\end{align*}
$$

Given that for [maximum likelihood](202210101331.md) we have

$$
\begin{align*}
p(x|\theta) &= \prod_{n=1}^{N} p(x^n|\theta) \\[0.5em] 
\Rarr \hat{\theta} &= \argmax_{\theta} \sum_{n=1}^{N} \log p(x^n|\theta)
\end{align*}
$$

We can see that maximising likelihood is equivalent to minimising the $KL$
divergence between the empirical distribution ($q$) and $p$.
