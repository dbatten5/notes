# Penalisation

A form of [regularisation](202210061507.md). A penalty is applied in the loss
function to deter the fitting process from bad actions.
$$
L(\bold{X}, \bold{y};\bold{\boldsymbol{\theta}}) = \underbrace{D(f_{\theta}(\bold{X},\bold{y}),\bold{y})}_{\text{"badness" of model}} + \lambda \overbrace{P(\boldsymbol{\theta})}^{\text{some penalty}}
$$

$\lambda$ is a [hyperparameter](202210061301.md) that sets the balance between the
two quantities.

For a [Ordinary least squares](202210111653.md) regression, the loss function with
a penalty could look like this: 
$$
L(\bold{X},\bold{y};\bold{w}) = \|\bold{X}\bold{w} - \bold{y}\|^2 + \lambda P(\bold{w})
$$

