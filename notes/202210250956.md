# Mixture of Gaussians (MoG)

The data are described as a weighted sum of $K$ [normal
distributions](202210101307.md)

$$
P(\bold{x}|\boldsymbol{\theta}) = \sum_{k=1}^{K} \lambda_k
\text{Norm}_{\bold{x}}\left[ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right]
$$

With $\lambda_1, \ldots, \lambda_k$ positive valued weights that sum to 1.
[Maximum likelihood](202210101331.md) doesn't work here as we have a summation
inside a logarithm, which doesn't have a simple solution. For a simpler
approach, we express the observed density as a marginalization and use the [EM
algorithm](202210250932.md) to learn the parameters.

We can express a MoG as the marginalization of a joint probability distribution
between the observed data $\bold{x}$ and a discrete hidden variable $h$ that
takes values $h \in \left\{ 1 \ldots K \right\}$:

$$
\begin{align*}
P(\bold{x}|h, \boldsymbol{\theta}) &= \text{Norm}_{\bold{x}}\left[ \boldsymbol{\mu}_h, \boldsymbol{\Sigma}_h \right] \\[0.5em]
P(h|\boldsymbol{\theta}) &= \text{Cat}_{h}\left[ \boldsymbol{\lambda} \right]
\end{align*}
$$

Where $\boldsymbol{\lambda} = [\lambda_1 \ldots \lambda_K]$ are the parameters
of the [categorical distribution](202210091049.md). Then we can recover the
original density using

$$
\begin{align*}
P(\bold{x}|\boldsymbol{\theta}) &= \sum_{k=1}^{K} P(\bold{x}, h = k | \boldsymbol{\theta}) \\[0.5em]
&= \sum_{k=1}^{K} P(\bold{x}|h=k,\boldsymbol{\theta})P(h=k|\boldsymbol{\theta}) \\[0.5em]
&= \sum_{k=1}^{K} \lambda_k \text{Norm}_{\bold{x}}\left[ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right]
\end{align*}
$$

The hidden variable $h$ determines which of the constituent normals is
*responsible* for the observed data point $\bold{x}$.

To learn the MoG parameters $\boldsymbol{\theta}
= \left\{ \lambda_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right\}_{k=1}^K$
we apply the EM algorithm. In the [E-step](202210250945.md) we maximize the bound
w.r.t. the distributions $q_i(h_i)$:

$$
\begin{align*}
q_i(h_i) = P(h_i = k|\bold{x}_i, \boldsymbol{\theta}^{[t]}) &= \ldots \\[0.5em]
&= \frac{\lambda_k \text{Norm}_{\bold{x}_i}\left[ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right]}
{\sum_{j=1}^{K} \lambda_j \text{Norm}_{\bold{x}_i}\left[ \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j \right]} \\[0.5em]
&= r_{ik}
\end{align*}
$$

That is, we compute the probability $P(h_i = k|\bold{x}_i,
\boldsymbol{\theta}^{[t]})$ that the $k^{th}$ normal distributions was
responsible for the  $i^{th}$ data point. This is called the *responsibility*
and is denoted by $r_{ik}$.

In the [M-step](202210250946.md) we maximize the bound w.r.t. the parameters
$\boldsymbol{\theta}
= \left\{ \lambda_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right\}_{k=1}^K$
so that

$$
\hat{\boldsymbol{\theta}}^{[t+1]} = 
\argmax_{\theta} \left[ 
\sum_{i=1}^{I} \sum_{k=1}^{K} r_{ik} \log [\lambda_k 
\text{Norm}_{\bold{x}_i}\left[ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right]
]
\right]
$$

