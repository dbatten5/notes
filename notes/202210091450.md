# Marginals & marginalisation

Given a [joint distribution](202210081156) $p(x,y)$, the marginal distribution
of $x$ is defined by
$$
\begin{align*}
p(x) &= \sum_{y} p(x,y) \enspace \text{ for discrete } \\[0.5em]
p(x) &= \int_{y} p(x,y)dy \enspace \text{ for continous }  \\[0.5em]
\end{align*}
$$
That is, the marginal distribution of one variable is obtained by summing over
all the possible values of the other variable. Here, $p(x)$ is termed a
*marginal* of the join probability distribution $p(x,y)$. The process of
computing a marginal is called *marginalisation*. This allows us to recover the
probability distribution of any single variable from a joint distribution. In
other words, we are finding the probability distribution of $x$ regardless of
(or in the absence of information about) the value of $y$.

Note that $p(x)$ is a distribution: it is non-negative and it also sums to 1:
$$
\sum_{x} p(x) = \sum_{x} \sum_{y} p(x,y) = 1
$$

More generally, we can define a marginal by summing over 1 of more variables,
$$
p(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n) = \sum_{x_i} p(x_1, \ldots, x_n)
$$

If for example we have 3 random variables $A, B, C$ and we denote $c$ for the
states of $C$, we can use marginalisation to determine that
$$
p(A|B) = \sum_{c} p(A,c|B)
$$

### Example

Suppose you have two sensors, a left sensor $l$ and a right sensor $r$. Each
sensor can be in an "off" or an "on" state. The probability distribution looks
like this
$$
\begin{aligned}
&\begin{array}{cccc}
& & \text{r} & & \\
& & \text{off} & \text{on} & \\
\text{l} & \text{off} & 0.1 & 0.6 \\
& \text{on} & 0.2 & 0.1 \\
\end{array}
\end{aligned}
$$
so, for example, $p(l=\text{off}, r=\text{on}) = 0.6$.

The marginal probabilities are:
$$
p(l) = [0.7, 0.3] \\[0.5em]
p(r) = [0.3, 0.7]
$$
