# Optimal hard margin

Good for [linearly separable](202210210913) data where we are looking to create
a hyperplane with a threshold at 0. Similar to [AdaBoost](202210170939), we use
a $y \in \left\{ 1, -1 \right\}$ class system (rather than $\left\{ 1, 0 \right\}$).
The intercept term is also separated out as a separate component, rather than
being incorporated into the model as a dummy feature in the weights vector at
$w_0$.

The model looks like this:

$$
\begin{equation}
\hat{y} = \begin{cases}
1 \hspace{1.5em} \text{ if } \bold{x} \cdot \bold{w} + b \ge 0 \\
-1 \hspace{0.8em}  \text{ otherwise}
\end{cases}
\end{equation}
$$

This will generate a hyperplane to separate the data. However, we would like to
maximise the distance from each cluster of training data to the hyperplane, i.e.
we would like to maximise the [margin](202210210946). We need a criterion to
encapsulate this:

$$
\text{maximise}_{\bold{w}, b} \hspace{0.5em} M \\[0.5em]
\text{subject to:} \\[0.5em]
\hspace{1em} y_i(\bold{x}_i \cdot \bold{w} + b) \ge M \\[0.5em]
\|\bold{w}\| = 1
$$

Rearranging give us:

$$
\begin{align*}
\frac{y_i(\bold{x}_i \cdot \bold{w} + b)}{\|\bold{w}\|} &\ge M \\[0.5em]
y_i(\bold{x}_i \cdot \bold{w} + b) &\ge M \|\bold{w}\|
\end{align*}
$$

This constraint has to hold for any magnitude of $\bold{w}$, we can therefore
set $\|\bold{w}\| = \frac{1}{M}$ to give us:

$$
\text{minimise}_{\bold{w}, b} \hspace{0.5em} \|\bold{w}\| \text{ (or } \frac{1}{2}\|\bold{w}\|^2) \\[0.5em]
\text{subject to: } y_i(\bold{x}_i \cdot \bold{w} + b) \ge 1 \\[0.5em]
$$

To incorporate the constraints, we use [Lagrange multipliers](202210211002):

$$
\begin{equation}
L_p = \frac{1}{2} \|\bold{w}\|^2 - \sum_{i}^{n} \alpha_i \left( y_i(\bold{x}_i \cdot \bold{w} + b) - 1 \right)
\end{equation}
$$

We then solve this for its stationary points:

$$
\begin{align*}
\nabla_{\bold{w}}L_p &= \bold{w} - \sum_{i}^{n} \alpha_i y_i \bold{x}_i \\[0.5em]
\bold{w} &= \sum_{i}^{n} \alpha_i y_i \bold{x}_i
\end{align*}
$$

Differentiating w.r.t. $b$ and setting to 0 gives us:

$$
\sum_{i}^{n} \alpha_i y_i = 0
$$

Incorporating into $(2)$ gives us:

$$
\begin{align*}
L_p = \sum_{i}^{n} \alpha_i - \frac{1}{2}\sum_{i}^{n} \sum_{j}^{n} \alpha_i \alpha_j y_i y_j (\bold{x}_i \cdot \bold{x}_j)
\end{align*}
$$

And we want to maximise over $\boldsymbol{\alpha}$.

Revisiting $(1)$ we get:

$$
\hat{y} = \begin{cases}
1 \hspace{1.5em} \text{ if } \sum_{i}^{n} \alpha_i y_i (\bold{x}_i \cdot \bold{x}) + b \ge 0 \\
-1 \hspace{0.8em}  \text{ otherwise}
\end{cases}
$$

We also get:

$$
\alpha_i \left[ y_i(\bold{x}_i \cdot \bold{w} + b) - 1 \right] = 0 \hspace{1em} \forall i
$$

This means that only the samples on the margin contribute to the solution, these
are known as [support vectors](202210211034)
