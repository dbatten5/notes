# AdaBoost

An [ensemble](202210141516) of [binary classification](202210141045) models but
instead of using 0 and 1 to classify, we use -1 and 1. AdaBoost also promotes
the use of "weak learners" (binary models with accuracy over 50%, i.e. slightly
better than random).

A classic model to use in the ensemble is a [decision stump](202210170946).

Each of the learners in the ensemble will be trained on the same dataset, but a
set of weights are applied to them to say which ones we care about most. So the
loss function that is being minimised for each learner is a weighted
missclassification error:

$$
\epsilon = \sum_{i} \omega_i \bold{1}(y_i \ne \hat{y}i)
$$

Initially the weights are all equal and normalised ($\frac{1}{n}$). Once the
learner is fitted it is assigned its prediction weight, i.e. how much of the
output prediction of the entire ensemble should be put on that learner:

$$
\alpha = \frac{1}{2}\log\left( \frac{1 - \epsilon}{\epsilon} \right)
$$

Note that each $\alpha$ is only based on its own individual error $\epsilon$,
not that of the entire ensemble.

After each learner is trained, the sample weights are re-weighted according to
the predictions of the most recent learner. There are two formulations for this:

$$
\begin{align*}
  \omega_i &\leftarrow \omega_ie^{\alpha \bold{1}(y_i \ne \hat{y}_i)} \\
  \omega_i &\leftarrow \omega_ie^{-\alpha y_i\hat{y}_i} \\[0.5em]
\end{align*}
$$

The effect of this is that the all of the samples that were classified
incorrectly have their weights increased and those classified correctly are down
weighted so that they are less important to the next classifier.

This process is repeated for all the learners in the ensemble until you end up
with a bunch of learners and a bunch of associated prediction weights
$\alpha_i$. The final prediction is then calculated like so:

$$
\hat{y} = \text{sgn}\sum_{t=1}^{k} \alpha_t h_t(\bold{x})
$$
