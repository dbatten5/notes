# Linear models

Assumption is all of the model variables are the inputs and the outputs are
continuous. The output is a weighted sum of the inputs with a possible intercept
term as well.

Linear models are often the simplest, but that doesn't make them any less
relevant than other more complicated models. Some relationships really can be
modelled as linear. Hooks Law is a classic example of a linear model.

Logarithms are used to convert exponential relationships to linear relationships.

### Example notation

Given $n$ training samples as $(\bold{x}_i, y_i)$ with $i \in \left\{ 1, \ldots, n \right\}$
where each $\bold{x}_i \in \R = \left( x_{i,1}, x_{i_2}, \ldots, x_{i,d} \right)$,
that is, each sample consists of a pair $\bold{x}$, a $d$-dimensional vector of $d$ real
numbers and $y$ which is a scalar real number.

We would like to find $\bold{w} \in \R^d = \left( w_1, w_2, \ldots, w_d \right)$
such that when you take the dot product of that vector with the input values of
any training sample, the output is a _good_ model of the data, $\hat{y} = f(\bold{x}) = \bold{x} \cdot \bold{w}$.
The hat over $y$ here means that it is an estimate.

Typically, all the of the training data is packed into a matrix:
$$
\bold{X} = \begin{bmatrix}
\bold{x}_1^T \\[0.5em]
\bold{x}_2^T \\
\vdots \\
\bold{x}_n^T \\
\end{bmatrix} = \begin{bmatrix}
x_{1,1} & x_{1,2} & \dots & x_{1,d} \\[0.5em]
x_{2,1} & x_{2,2} & \dots & x_{2,d} \\[0.5em]
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \dots & x_{n,d} \\[0.5em]
\end{bmatrix} \enspace \enspace \bold{y} = \begin{bmatrix}
y_1 \\[0.5em]
y_2 \\[0.5em]
\vdots \\
y_n \\
\end{bmatrix}
$$

Here, $\bold{X}$ is known as the design, or model, matrix. The columns are known
as the _feature dimensions_ and the rows are known as the *samples*. Can then
say that $\bold{\hat{y}} = \bold{X}\bold{w}$. Currently this model does not have
any intercept, or bias. This can be added manually as $\bold{\hat{y}} =
\bold{X}\bold{w} + b$. Alternatively, the bias can be added to $\bold{w}$ as
$w_0 = b$ and all the input features will be augemnted with a dummy feature $x_0 = 1$.

Can think of $\bold{w}$ as a line of best fit in 1 dimension (becomes a plane of
best fit in 2 dimensions).

In order to determine whether our $\bold{w}$ is "good" or "bad", we need some
idea of a [loss function](202210061216). A simple loss function in this case is
the [Residual sum of squares](202210111531). Convenient to consider the Euclidean
distance between the observed output vs the predicted output as a form of loss
function, $\|\bold{y} - \bold{\hat{y}}\|^2$. This is known as the Euclidean, or
L2, norm. To get our loss function, we sum the norms as 
$$
\sum_{n=1}^{n} \|y_i - \hat{y}_i\|^2 = \sum_{n=1}^{n} \|y_i - \bold{x}_i \cdot
\bold{w}\|^2 = \|\bold{X}\bold{w} - \bold{y}\|^2
$$
This is known as the [Residual sum of squares](202210111531) and to solve it we
solve 
$$
\bold{w}^* = \argmin_{\bold{w}} \|\bold{X}\bold{w} - \bold{y}\|^2
$$
