# Linear models

Assumption is all of the model variables are the inputs and the outputs are
continuous. The output is a weighted sum of the inputs with a possible intercept
term as well.

Linear models are often the simplest, but that doesn't make them any less
relevant than other more complicated models. Some relationships really can be
modelled as linear. Hooks Law is a classic example of a linear model.

Logarithms are used to convert exponential relationships to linear relationships.

Note that linear models don't have to fit straight lines. A model is defined as
linear if it is linear in the parameters, so all the terms in the model are
either a constant or a *parameter* multiplied by an independent variable. So for 
$$
Y = \underbrace{\beta_0}_{\text{constant}} +  \underbrace{\beta_1}_{\text{parameter}}X_1 + \beta_2X_2 + \ldots +
\beta_kX_k
$$
we can see that it follows the form:

Dependent variable = constant + parameter * independent variable + ... +
parameter * IV

and is therefore linear. We can use this model to fit a curve by doing something
like $Y = \beta_0 + \beta_1X1 + \beta_2X_1^2$.


### Example notation

Given $n$ training samples as $(\bold{x}_i, y_i)$ with $i \in \left\{ 1, \ldots, n \right\}$
where each $\bold{x}_i \in \R = \left( x_{i,1}, x_{i_2}, \ldots, x_{i,d} \right)$,
that is, each sample consists of a pair $\bold{x}$, a $d$-dimensional vector of $d$ real
numbers and $y$ which is a scalar real number.

We would like to find $\bold{w} \in \R^d = \left( w_1, w_2, \ldots, w_d \right)$
such that when you take the dot product of that vector with the input values of
any training sample, the output is a _good_ model of the data, $\hat{y} = f(\bold{x}) = \bold{x} \cdot \bold{w}$.
The hat over $y$ here means that it is an estimate.

Typically, all the of the training data is packed into a matrix:
$$
\bold{X} = \begin{bmatrix}
\bold{x}_1^T \\[0.5em]
\bold{x}_2^T \\
\vdots \\
\bold{x}_n^T \\
\end{bmatrix} = \begin{bmatrix}
x_{1,1} & x_{1,2} & \dots & x_{1,d} \\[0.5em]
x_{2,1} & x_{2,2} & \dots & x_{2,d} \\[0.5em]
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \dots & x_{n,d} \\[0.5em]
\end{bmatrix} \enspace \enspace \bold{y} = \begin{bmatrix}
y_1 \\[0.5em]
y_2 \\[0.5em]
\vdots \\
y_n \\
\end{bmatrix}
$$

Here, $\bold{X}$ is known as the design, or model, matrix. The columns are known
as the _feature dimensions_ and the rows are known as the *samples*. Can then
say that $\bold{\hat{y}} = \bold{X}\bold{w}$. Currently this model does not have
any intercept, or bias. This can be added manually as $\bold{\hat{y}} =
\bold{X}\bold{w} + b$. Alternatively, the bias can be added to $\bold{w}$ as
$w_0 = b$ and all the input features will be augmented with a dummy feature $x_0 = 1$.

Can think of $\bold{w}$ as a line of best fit in 1 dimension (becomes a plane of
best fit in 2 dimensions).

In order to determine whether our $\bold{w}$ is "good" or "bad", we need some
idea of a [loss function](202210061216.md). A common loss function in this case is
the [Residual sum of squares](202210111531.md). 

Often we need to consider an [error term](202210111636.md) $\epsilon$ to
encapsulate a variety of different errors that may arise when creating a model,
$\bold{y} = \bold{X}\bold{w} + \epsilon$.
