# Factor analysis

Often with visual data we are dealing with very large dimensions of data. To
model this with an [multivariate normal distribution](202210101307.md) for example,
would require a very large [covariance](202210130950.md) matrix which has its
implications in memory and in computing the inverse for the normal likelihood.
One could use a diagonal matrix for the covariance but this assumes that each
dimension of the data is independent (unlikely for an image of a face as
neighbouring pixels in the same region of the face would covary very closely).

Factor analysis provides a compromise in which the covariance matrix is
structured so that it contains fewer unknown parameters than the full matrix but
more than the diagonal form. It models part of the high-dimensional space with a
full model and mops up the remaining variation with a diagonal model.

A factor analyzer describes a [linear subspace](202210251144.md) with a full
covariance model. A $D$-dimensional space contains subspaces of dimensions $1,
2, \ldots, D - 1$.

The PDF is given by

$$
P(\bold{x}) = \text{Norm}_{\bold{x}}\left[
\boldsymbol{\mu}, \boldsymbol{\Phi}\boldsymbol{\Phi}^{\top} + \boldsymbol{\Sigma}
\right]
$$

Here, $\boldsymbol{\Phi}\boldsymbol{\Phi}^{\top}$ describes a full covariance
model over the subspace: the $K$ columns of the rectangular matrix
$\boldsymbol{\Phi} = \left[ \phi_1, \phi_2, ..., \phi_K \right]$ are termed
*factors* which are basis vectors that determine the subspace modeled. The
factors will span the set of directions where the data covary the most.
$\boldsymbol{\Sigma}$ is a diagonal matrix that accounts for all remaining
variation.

### Marginalisation

We can view the factor analysis model as a [marginalisation](202210091450.md) of
a joint diagonal between the observed data $\bold{x}$ and a $K$-dimensional
hidden variable $\bold{h}$.

$$
\begin{align*}
P(\bold{x}|\bold{h}) &= \text{Norm}_{\bold{x}}\left[ \boldsymbol{\mu} + \boldsymbol{\Phi}\bold{h}, 
\boldsymbol{\Sigma} \right] \\[0.5em]
P(\bold{h}) &= \text{Norm}_{\bold{h}}\left[ \bold{0}, \bold{I} \right] \\[0.5em]
\end{align*}
$$

This leads us to interpret $\bold{h}$ thusly: each element $h_k$ weights the
associated basis function $\phi_k$ in the matrix $\boldsymbol{\Phi}$ and hence
defines a point on the subspace.

We can use the [Expectation maximization](202210250932.md) to learn the parameters 
$\boldsymbol{\theta} = \left\{ \boldsymbol{\mu}, \boldsymbol{\Phi}, \boldsymbol{\Sigma} \right\}$
