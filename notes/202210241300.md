# Bayesian regression

We compute the probability distribution over possible values of the parameters
$\boldsymbol{\phi}$. When we evaluate the probability of new data, we take a
weighted average of the predictions induced by the different possible values.

From [linear regression](202210241230.md), we have:

$$
\tag{Likelihood} P(\bold{w}|\bold{X}, \boldsymbol{\theta}) =
\text{Norm}_{\bold{w}}\left[ \bold{X}^{\top}\boldsymbol{\phi},
\sigma^2 \bold{I} \right]
$$

$$
\tag{Prior} P(\boldsymbol{\phi}) = \text{Norm}_{\phi}\left[ 0, \sigma^2_p \bold{I} \right]
$$

Using [Bayes' rule](202210090920.md) to calculate the posterior:

$$
P(\boldsymbol{\phi}| \bold{X}, \bold{w}) =
\frac{P(\bold{w}|\bold{X}, \phi)P(\phi|\bold{X})}{P(\bold{w}|\bold{X})}
$$

So the [posterior](202210121758.md):

$$
P(\phi|\bold{X}, \bold{w}) = \text{Norm}_{\phi}\left[
\frac{1}{\sigma^2}\bold{A}^{-1}\bold{X}\bold{w}, \bold{A}^{-1}
\right]
$$

Where

$$
\begin{equation}
\bold{A} = \frac{1}{\sigma^2}\bold{X}\bold{X}^{\top} + \frac{1}{\sigma_p^2}\bold{I}
\end{equation}
$$

With $\sigma_p^2$ being the variance on the prior model.

For inference:

$$
P(\bold{w}^* | \bold{x}^*, \bold{X}, \bold{w}) =
\text{Norm}_{w^*}\left[
\frac{1}{\sigma^2}\bold{x}^{*\top}\bold{A}^{-1}\bold{X}\bold{w},
\bold{x}^{*\top}\bold{A}^{-1}\bold{x}^* + \sigma^2
\right]
$$

For `numpy`, suppose we have a $2 \times \text{number of data points}$ matrix
where the first row is all `1`:

```python
mean = (1 / sigmaSq) * X_star[:, cX].T @ inv(A) @ X @ w
var = X_star[:,cX].T @ inv(A) @ X_star[:, cX] + sigmaSq
```

This uses the array slicing notation from `numpy` where `x[:, idx]` takes
everything from the `idx`$^{th}$ column.

Note that for $(1)$, we are calculating an inverse of a matrix. For large
dimensions, this can be expensive. We can use the
[Woodbury identity](202211011200.md) to simplify the inversion:

$$
A^{-1} = \sigma^2_p \bold{I}_D - \sigma^2_p \bold{X} \left( 
\bold{X}^{\top}\bold{X} + \frac{\sigma^2}{\sigma^2_p} \bold{I}
\right)^{-1} \bold{X}^{\top}
$$

This reduces the inversion from $D \times D$ to $I \times I$, useful for
situations where we have a lower number of examples than dimensions.
