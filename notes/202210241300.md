# Bayesian regression

From [linear regression](202210241230), we have:

$$
\tag{Likelihood} P(\bold{w}|\bold{X}, \boldsymbol{\theta}) =
\text{Norm}_{\bold{w}}\left[ \bold{X}^{\top}\boldsymbol{\phi},
\sigma^2 \bold{I} \right]
$$

$$
\tag{Prior} P(\boldsymbol{\phi}) = \text{Norm}_{\phi}\left[ 0, \sigma^2_p \bold{I} \right]
$$

Using [Bayes' rule](202210090920):

$$
P(\boldsymbol{\phi}| \bold{X}, \bold{w}) =
\frac{P(\bold{w}|\bold{X}, \phi)P(\phi|\bold{X})}{P(\bold{w}|\bold{X})}
$$

So the [posterior](202210121758):

$$
P(\phi|\bold{X}, \bold{w}) = \text{Norm}_{\phi}\left[
\frac{1}{\sigma^2}\bold{A}^{-1}\bold{X}\bold{w}, \bold{A}^{-1}
\right]
$$

Where

$$
\bold{A} = \frac{1}{\sigma^2}\bold{X}\bold{X}^{\top} + \frac{1}{\sigma_p^2}\bold{I}
$$

For inference:

$$
P(\bold{w}^* | \bold{x}^*, \bold{X}, \bold{w}) =
\text{Norm}_{w^*}\left[
\frac{1}{\sigma^2}\bold{x}^{*\top}\bold{A}^{-1}\bold{X}\bold{w},
\bold{x}^{*\top}\bold{A}^{-1}\bold{x}^* + \sigma^2
\right]
$$

