# Binary classification

Unlike [regression](202210061318) for linear models, binary classification is
when the outputs fall into 2 classes with either a 0 or 1 value. For example,
for 1D binary classification, we could say that if $x < 0$, then $y = 0$, and if
$x > 0$, then $y = 1$.

In linear regression, we are interested in the hyperplane that describes the
trend. In binary classification, we are instead interested in the two half
spaces that the hyperplane divides the feature space into.

The [residual sum of squares](202210111531) is not an appropriate loss function
for these problems, since it cares about distance from the trend line. In
classification, we are only concerned with data being in the correct
classification, not with how "far" into the correct classification. So for the
example above, consider a data set that has a $x = 1$ and a $x = 100$. Both are
classified as $y = 1$ and that's all we're concerned with. The residual sum of
squares will be skewed by the fact that the $x = 100$ is "far" into the
classification. The square in the RSOS also gives more weight to outliers, again
not important for binary classification.

Putting it all together, we get
$$
\hat{y} = \begin{cases}
  1 &\text{if } \bold{x} \cdot \bold{w} \ge 0 \\
  0 &\text{otherwise }
\end{cases}
$$

If we can't use RSOS as a loss, then what can we use? An intuitive one is the
[zero-one loss](202210141113), however this is non-convex and non-differentiable
and it's hard to optimise as a result. Instead we'd like something smooth and
continuous that captures the idea of being mostly 0 or mostly 1. Since
probabilities are in $[0,1]$, that could be a good use case. Essentially this
would say that "its $x$ amount likely that this example is a 0 (or 1)":

$$
\begin{align*}
  P(y = 1|\bold{x}; \bold{w}) &= f_{\bold{w}}(x) \\[0.5em]
  P(y = 0|\bold{x}; \bold{w}) &= 1 - f_{\bold{w}}(x) \\[0.5em]
\end{align*}
$$

Would like to use a linear function for $f_\bold{w}(x)$, but linear inputs are
continuous and unbounded, whereas probabilities need to be constrained between 0
and 1. Can use the [sigmoid (or logistic) function](202210141123) to achieve
this, which has the effect of compressing the domain of $\R$ into
$[0,1]$ in a smooth and continuous way. So

$$
\begin{align*}
P(y = 1 | \bold{x}; \bold{w}) &= \sigma(\bold{x} \cdot \bold{w}) \\[0.5em]
P(y = 0 | \bold{x}; \bold{w}) &= 1 - \sigma(\bold{x} \cdot \bold{w})
\end{align*}
$$

That is, the probability that $y = 1$ conditioned on $\bold{x}$ and
parameterised by $\bold{w}$ is given by the sigmoid of $\bold{x} \cdot \bold{w}$.

Combining the equations we get
$$
P(y \medspace | \medspace \bold{x}; \bold{w}) =
\sigma(\bold{x} \cdot \bold{w})^y(1 - \sigma(\bold{x} \cdot \bold{w}))^{1-y}
$$

For the whole training set:
$$
\underbrace{P(y \medspace | \medspace \bold{X}; \bold{w})}_{\text{likelihood } L(\bold{w})} =
\prod_{i}^{n}  \sigma(\bold{x_i} \cdot \bold{w})^{y_i}(1 - \sigma(\bold{x_i} \cdot \bold{w}))^{1-y_i}
$$

So to find out $\bold{w}^*$ we find
$$
\begin{align*}
\bold{w}^* &= \argmax_{\bold{w}} L(\bold{w}) \\
\vdots \\
\nabla_{\bold{w}}l &= \bold{\bold{X}}^{\top}(\bold{y} - \bold{\hat{y}}) \\[0.5em]
&= \bold{\bold{X}}^{\top}(\bold{y} - \sigma(\bold{\bold{X}\bold{w}})) \\[0.5em]
\end{align*}
$$

Setting this to 0, this function can't be solved, but we do have a gradient now
which can be used to find a [numerical](202210061251) solution.

For many classes, we need to use a [multiclass](202210141248) approach
