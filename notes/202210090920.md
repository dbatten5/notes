# Bayes' rule

$$
\begin{align*}
P(y|x)P(x) &= P(x|y)P(y) \\[1em]
P(y|x) &= \frac{P(x|y)P(y)}{P(x)} \medspace \text{ or } \medspace \frac{P(x,y)}{P(x)}\\[1em]
&= \frac{P(x|y)P(y)}{\int P(x,y)dy} \\[1em]
&= \frac{P(x|y)P(y)}{\int P(x|y)P(y)dy} \\[1em]
\end{align*}
$$

## Terminology

- **Likelihood** - the propensity for observing a certain value of $x$ given a
  certain value of $y$.
- **Prior** - what we know about $y$ before seeing $x$.
- **Evidence** - a constant to ensure that the left hand side is the valid
  distribution.
- **Posterior** - what we know about $y$ after seeing $x$.

$$
\underbrace{P(y|x)}_{\text{posterior}} = \frac{\overbrace{P(x|y)}^{\text{likelihood}}\overbrace{P(y)}^{\text{prior}}}
{\underbrace{\int P(x|y)P(y)dy}_{\text{evidence}}}
$$

With $y =$ some variable $\theta$, and $x =$ observed data $\mathcal{D}$ and given
a model $p(\mathcal{D}|\theta)$ and a prior belief $p(\theta)$ about which
variables are appropriate, we can infer a posterior $p(\theta|\mathcal{D})$,
meaning the probability of observing $\theta$ given the observed data
