# Bootstrap

Assuming that our data is representative of a population, we consider that the
sample we have taken is the whole population. We can then employ the methods
listed in [confidence intervals](202210201346) to take samples from our samples.
One was to achieve this is with [sampling with replacement](202210201523). This
is used to generate re-samples of the same size as the original sample.

Bootstrapping provides a way to mimic the data generating process and provides
us information about the sampling variability.

**Why re-sample?** Very easy to do, doesn't require fetching new data (could be
expensive, time consuming etc.). We can then estimate moments of sampling
distributions using samples.

### Using the bootstrap

The simplest way to use the bootstrap is to calculate the
[variance](202210081517) of a [statistic](202210101627) of interest along with
the approximation given by the [CLT](202210151509):

$$
\tag{1.0} \left[
\overline{X} + z_{0.0025}\hat{se}_{boot},
\overline{X} + z_{0.975}\hat{se}_{boot},
\right]
$$

This gives us a 95% [confidence interval](202210201346) for the mean, where
$\hat{se}_{boot}$ is the standard error ("deviation") obtained by the bootstrap.

### The (Nonparametric) Bootstrap

1. Draw $X_1^*, \ldots, X_n^* \backsim \hat{F}_n$ 
1. Compute $\overline{X}_n^*$ by averaging $X_1^*, \ldots, X_n^*$
1. Repeat steps 1 and 2 $B$ times to get $\overline{X}_{n,1}^*, \ldots, \overline{X}_{n,B}^*$
1. Let

$$
s.e._{boot} \equiv \sqrt{\frac{1}{B}\sum_{b=1}^{B} \left( \overline{X}_{n,b}^* -
\frac{1}{B}\sum_{r=1}^{B} \overline{X}_{n,r}^* \right)^2}
$$

Ideally $B$ to be all possible combinations of the sample, but that's often not
feasible. It's sufficient to just make $B$ as large as possible.

We can generalise this to any statistic $T$ with the exact same formula above,
just replacing $\overline{X}$ with $T$ and then we drop $s.e._{boot}$ into $(1.0)$.
