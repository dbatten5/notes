# Generative model

This type of model concerns modelling the likelihood $P(\bold{x}|\bold{w})$ (or
$P(\bold{x}, \bold{w})$).

## $P(\bold{x}|\bold{w})$

How to model $P(\bold{x}|\bold{w})$?

1. Choose an appropriate form of $P(\bold{x})$
1. Make parameters a function of $\bold{w}$
1. Function takes parameters $\boldsymbol{\theta}$ that define its shape

Since the distribution $P(\bold{x})$ now depends on both the world state and
these parameters, we write it as $P(\bold{x}|\bold{w},\bold{\theta})$ and refer
to it as the _likelihood_.

**Learning algorithm**: learn parameters $\boldsymbol{\theta}$ from training data $\bold{x}, \bold{w}$

**Inference algorithm**: compute $P(\bold{w}|\bold{x})$ using [Bayes' rule](202210090920)
using $P(\bold{w})$ as a [prior](202210111029).

### Regression

Since the data are univariate and continuous, we will model the data as a normal
distribution with fixed variance, $\sigma^2$, and a mean $\mu$ that is a linear
function $\phi_0 + \phi_1 w$ of the world state, so that  

$$
P(x|w, \boldsymbol{\theta}) = \text{Norm}_x\left[ \phi_0 + \phi_1w, \sigma^2 \right]
$$

We also need a prior $P(w)$ over the world states, which might also be normal 
$$
P(w) = \text{Norm}_w\left[ \mu_p, \sigma_{p}^2 \right]
$$

The learning algorithm fits the parameters $\boldsymbol{\theta} = \left\{
\phi_0, \phi_1, \sigma^2 \right\}$ using the training data $\bold{x}, \bold{w}$
and fits the parameters $\boldsymbol{\theta}_p = \left\{ \mu_p, \sigma_{p}^2 \right\}$ 
using the world states $\bold{w}$.

### Classification

We allow the variance $\sigma^2$ and the mean $\mu$ of the norm of the data $x$
to be functions of the binary world state $w$, so that the likelihood is 

$$
P(x|w, \boldsymbol{\theta}) = \text{Norm}_x\left[ \mu_w, \sigma_{w}^2 \right] 
$$

This means that we have one set of parameters $\{\mu_0, \sigma_{0}^2\}$ when the
state of the world is $w = 0$ and a different set $\left\{ \mu_1, \sigma_{1}^2 \right\}$
when $w = 1$, so  
$$
P(x|w = 0) = \text{Norm}_x \left[ \mu_0, \sigma_{0}^2 \right]  \\[0.5em]
P(x|w = 1) = \text{Norm}_x \left[ \mu_1, \sigma_{1}^2 \right] 
$$

We also define a prior $P(w)$ over the world state 
$$
P(w) = \text{Bern}_w\left[ \lambda_p \right] 
$$

where $\lambda_p$ is the prior probability of observing the state $w = 1$. We
fit $\boldsymbol{\theta} = \left\{ \mu_0, \sigma_{0}^2, \mu_1, \sigma_{1}^2 \right\}$ 
first for $w = 0$ and then for $w = 1$, and learn $\lambda_p$ from $\bold{w}$.

The inference algorithm takes new datum $x$ and returns the posterior
distribution $P(w|x, \boldsymbol{\theta})$ over the world state $w$ using
Bayes':

$$
P(w|x) = \frac{P(x|w)P(w)}{\sum_{w=0}^{1} P(x|w)P(w)}
$$


## $P(\bold{x},\bold{w})$

How to model $P(\bold{x},\bold{w})$?
1. Concatenate $\bold{x}$ and $\bold{w}$ to make $\bold{z}=[\bold{x}^{\top}\bold{w}^{\top}]^{\top}$
1. Model the pdf of $\bold{z}$
1. Pdf takes parameters $\boldsymbol{\theta}$ that define its shape

**Learning algorithm**: learn parameters $\boldsymbol{\theta}$ from training data $\bold{x}, \bold{w}$

**Inference algorithm**: compute $P(\bold{w}|\bold{x})$ using [Bayes' rule](202210090920)

### Regression

1. Concatenate $\bold{x}$ and $\bold{w}$ to make $\bold{z = [\bold{x}^{\top}\bold{w}^{\top}]^{\top}}$
2. Model the pdf of $\bold{z}$ as a normal distribution
3. Pdf takes parameters $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ that define
   its shape

$$
P(\bold{w}|\bold{x}) = \frac{P(\bold{x}, \bold{w})}{P(\bold{x})} =
\frac{P(\bold{x},\bold{w})}{\int P(\bold{x},\bold{w})d\bold{w}  }
$$

