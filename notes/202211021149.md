# Steepest descent

A form of [gradient based optimization](202211011714.md), we measure the
gradient and select the direction which moves us downhill fastest. We then move
in this direction until the function no longer decreases, then recompute the
steepest direction and move again. We stop when the gradient is 0 and the second
derivative is positive.

$$
\boldsymbol{\theta}^{\left[ t + 1 \right]} = \boldsymbol{\theta}^{\left[ t \right]} - 
\lambda \frac{\partial f}{\partial \boldsymbol{\theta}}
\bigg|_{\boldsymbol{\theta}^{\left[ t \right]}}
$$

The derivative $\partial f / \partial \boldsymbol{\theta}$ is the gradient
vector (the Jacobian).
