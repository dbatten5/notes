# Multivariate normal distribution

The PDF is given as:

$$
P(\bold{x}) = 
\frac{1}{(2\pi)^{D/2}|\boldsymbol{\Sigma}|^{1/2}}
\exp \left[
-0.5(\bold{x} - \boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1} (\bold{x} - \boldsymbol{\mu}) 
\right]
$$

**NB** when writing this out in python make sure to use matrix multiplication 
(`@`) for $(\bold{x} - \boldsymbol{\mu})^{\top} \text{(@)} \boldsymbol{\Sigma}^{-1} \text{(@)} (\bold{x} - \boldsymbol{\mu})$.

Shortened as:

$$
P(\bold{x}) = \text{Norm}_{\bold{x}}[\boldsymbol{\mu}, \bold{\Sigma}]
$$

- $\boldsymbol{\mu}$ is a $D \times 1$ vector that describes the mean of the
  distribution, where $D$ is the number of dimensions.
- $\bold{\Sigma}$ is a symmetric, [positive definite](202210131406.md) $D \times D$ matrix, known as the [covariance](202210130950.md) matrix. This determines the shape of its iso-contours (rings).

Its hyperparameter distribution is modelled by the [Normal Inverse Wishart Distribution](202210101311.md).

When we [marginalize](202210091450.md) or take the [conditional](202210071001.md)
distribution of a normal with respect to a subset of variables, the result is
another normal.

### Limitations

Often a normal distribution is not appropriate for modelling real world
situations, for 3 reasons:
- it's unimodal, often real images are not well represented by a pdf with a
  single peak.
- not robust, a single outlier can dramatically affect the estimates of the mean
  and the [covariance](202210130950.md).
- has too many parameters, for a D = e.g. 10800 $\bold{x}$, the covariance
  matrix contains $D(D+1)$/2 parameters. If you have fewer training examples,
  you won't be able to specify the parameters uniquely and will be forced to use
  a diagonal form.

[Hidden variables](202210190852.md) are used to solve this issue.
