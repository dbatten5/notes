# Mini-batch gradient descent

Halfway between [batch](202210141216) and [stochastic](202210141221), don't
process all samples at the same time, but similarly don't do just 1 sample at a
time. Divide the training set into chunks, then process each chunk at a time.
This introduces another hyperparameter, the chunk size.
