# Ridge regression

A form of [penalisation](202210111821.md). A penalty applied to the model weights
takes the same form as the penalty on the residuals: $P(\bold{w}) =
\|\bold{w}\|^2$. This essentially means, make the weights as small as possible.

The updated loss function for a [ordinary least squares](202210111653.md) with this
penalty term is:
$$
\begin{equation}
\bold{w}^* = \argmin_{\bold{w}} \|\bold{X}\bold{w} - \bold{y}\|^2 + \lambda \|\bold{w}\|^2
\end{equation}
$$

Solving for $\bold{w}^*$ gives us:
$$
\bold{w}^* = (\bold{X}^{\top}\bold{X} + \lambda \bold{I})^{-1}\bold{X}^{\top}\bold{y}
$$

In statistical form, we can write $(1)$ as

$$
\sum_{i=1}^{n} \left( 
y^{(i)} - \beta_0 - \sum_{j=1}^{p} \beta_jx^{(i)}_j
\right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

Running this without considering the format of the data might give suboptimal
results, since regression with shrinkage is **scale-dependent**.

Consider the standardization:

$$
\overline{x}^{(i)}_j \equiv \frac{x_j^{(i)}}{S_j}
$$

where

$$
S_j = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (x_j^{(i)} - \overline{x}_j)^2}
$$

