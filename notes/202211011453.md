# Dual linear regression

In standard [linear regression](202210241230.md), $\boldsymbol{\phi}$ contains
$D$ entries corresponding to each of the $D$ dimensions of the input data.

We can re-parameterize the model in terms of a vector $\boldsymbol{\psi}$ which
has $I$ entries where $I$ is the number of training examples. This is more
efficient in situations where we are training a model where the input data are
high dimensional, but the number of examples is small ($I < D$).

With

$$
P(w_i|\bold{x}_i) = \text{Norm}_{\bold{x}_i}\left[
\boldsymbol{\phi}^{\top}\bold{x}_i, \sigma^2
\right]
$$

We represent $\boldsymbol{\phi}$ as a weighted sum of the observed data points:

$$
\boldsymbol{\phi} = \bold{X}\boldsymbol{\psi}
$$

Where $\boldsymbol{\psi}$ is a $I \times 1$ vector representing the weights.
This is known as *dual parameterization*.

For $D < I$, we can find parameters $\boldsymbol{\psi}$ to represent any
gradient vector $\boldsymbol{\phi}$.

The regression model becomes:

$$
P(\bold{w}|\bold{X}, \boldsymbol{\theta}) =
\text{Norm}_{\bold{w}}\left[
\bold{X}^{\top}\bold{X}\boldsymbol{\psi}, \sigma^2\bold{I}
\right]
$$

With $\boldsymbol{\theta} = \left\{ \boldsymbol{\psi}, \sigma^2 \right\}$.

Using [maximum likelihood](202210101331.md) to find the parameters:

$$
\begin{align*}
\hat{\boldsymbol{\psi}} &= (\bold{X}^{\top}\bold{X})^{-1}\bold{w} \\[0.5em]
\hat{\sigma}^2 &= \frac{
(\bold{w} - \bold{X}^{\top}\bold{X}\boldsymbol{\psi})^{\top}
(\bold{w} - \bold{X}^{\top}\bold{X}\boldsymbol{\psi})
}
{I} \\[0.5em]
\end{align*}
$$

For the [Bayesian](202210111043.md) approach, we compute the posterior
distribution $P(\boldsymbol{\psi}|\bold{X},\bold{w})$ over possible values of
$\boldsymbol{\psi}$. Since we have no particular prior knowledge, we choose a
normal distribution with a large spherical covariance:

$$
P(\boldsymbol{\psi}) = \text{Norm}_{\psi} \left[\bold{0}, \sigma^2_p \bold{I} \right]
$$

Therefore the posterior is given by

$$
P(\boldsymbol{\psi}|\bold{X},\bold{w},\sigma^2) = 
\text{Norm}_{\psi} \left[ 
\frac{1}{\sigma^2}\bold{A}^{-1}\bold{X}^{\top}\bold{X}\bold{w},\bold{A}^{-1}
\right]
$$

Where

$$
\bold{A} = \frac{1}{\sigma^2}\bold{X}^{\top}\bold{X}\bold{X}^{\top}\bold{X} +
\frac{1}{\sigma^2_p}\bold{I}
$$

And the predictive distribution:

$$
P(w^*|\bold{x}^*,\bold{X},\bold{w}) =
\text{Norm}_{w^*} \left[ 
\frac{1}{\sigma^2}\bold{x}^{*\top}\bold{X}\bold{A}^{-1}\bold{X}^{\top}\bold{X}\bold{w}^*,
\bold{x}^{*\top}\bold{X}\bold{A}^{-1}\bold{X}^{\top}\bold{x}^* + \sigma^2
\right]
$$

