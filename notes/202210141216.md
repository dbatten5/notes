# Batch gradient descent

Starting at a random initial position, we calculate the local loss gradient with
respect to the entire training dataset. We take a small step, known as a
*learning rate* down the gradient, check if the minimum has been reached, if not
then do it all again. Comes with certain problems:

- need to pick a learning rate
- need to pick a starting point
- can get stuck in local minima
- can be prohibitively expensive for large datasets because we need to calculate
  the loss gradient for all the samples in the training set.

To get around the last issue, we can use [stochastic gradient descent](202210141221.md).
